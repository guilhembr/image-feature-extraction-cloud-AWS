{"cells": [{"metadata": {}, "id": "72bfb53e", "cell_type": "markdown", "source": "# D\u00e9ployez un mod\u00e8le dans le cloud\n\n\n# Sommaire :\n\n**1. Pr\u00e9ambule**<br />\n&emsp;1.1 Probl\u00e9matique<br />\n&emsp;1.2 Objectifs dans ce projet<br />\n&emsp;1.3 D\u00e9roulement des \u00e9tapes du projet<br />\n**2. Choix techniques g\u00e9n\u00e9raux retenus**<br />\n&emsp;2.1 Calcul distribu\u00e9<br />\n&emsp;2.2 Transfert Learning<br />\n**3. D\u00e9ploiement de la solution en local**<br />\n&emsp;3.1 Environnement de travail<br />\n&emsp;3.2 Installation de Spark<br />\n&emsp;3.3 Installation des packages<br />\n&emsp;3.4 Import des librairies<br />\n&emsp;3.5 D\u00e9finition des PATH pour charger les images et enregistrer les r\u00e9sultats<br />\n&emsp;3.6 Cr\u00e9ation de la SparkSession<br />\n&emsp;3.7 Traitement des donn\u00e9es<br />\n&emsp;&emsp;3.7.1 Chargement des donn\u00e9es<br />\n&emsp;&emsp;3.7.2 Pr\u00e9paration du mod\u00e8le<br />\n&emsp;&emsp;3.7.3 D\u00e9finition du processus de chargement des images et application <br />\n&emsp;&emsp;&emsp;&emsp;&emsp;de leur featurisation \u00e0 travers l'utilisation de pandas UDF<br />\n&emsp;&emsp;3.7.4 Ex\u00e9cution des actions d'extractions de features<br />\n&emsp;3.8 Chargement des donn\u00e9es enregistr\u00e9es et validation du r\u00e9sultat<br />\n**4. D\u00e9ploiement de la solution sur le cloud**<br />\n&emsp;4.1 Choix du prestataire cloud : AWS<br />\n&emsp;4.2 Choix de la solution technique : EMR<br />\n&emsp;4.3 Choix de la solution de stockage des donn\u00e9es : Amazon S3<br />\n&emsp;4.4 Configuration de l'environnement de travail<br />\n&emsp;4.5 Upload de nos donn\u00e9es sur S3<br />\n&emsp;4.6 Configuration du serveur EMR<br />\n&emsp;&emsp;4.6.1 \u00c9tape 1 : Logiciels et \u00e9tapes<br />\n&emsp;&emsp;&emsp;4.6.1.1 Configuration des logiciels<br />\n&emsp;&emsp;&emsp;4.6.1.2 Modifier les param\u00e8tres du logiciel<br />\n&emsp;&emsp;4.6.2 \u00c9tape 2 : Mat\u00e9riel<br />\n&emsp;&emsp;4.6.3 \u00c9tape 3 : Param\u00e8tres de cluster g\u00e9n\u00e9raux<br />\n&emsp;&emsp;&emsp;4.6.3.1 Options g\u00e9n\u00e9rales<br />\n&emsp;&emsp;&emsp;4.6.3.2 Actions d'amor\u00e7age<br />\n&emsp;&emsp;4.6.4 \u00c9tape 4 : S\u00e9curit\u00e9<br />\n&emsp;&emsp;&emsp;4.6.4.1 Options de s\u00e9curit\u00e9<br />\n&emsp;4.7 Instanciation du serveur<br />\n&emsp;4.8 Cr\u00e9ation du tunnel SSH \u00e0 l'instance EC2 (Ma\u00eetre)<br />\n&emsp;&emsp;4.8.1 Cr\u00e9ation des autorisations sur les connexions entrantes<br />\n&emsp;&emsp;4.8.2 Cr\u00e9ation du tunnel ssh vers le Driver<br />\n&emsp;&emsp;4.8.3 Configuration de FoxyProxy<br />\n&emsp;&emsp;4.8.4 Acc\u00e8s aux applications du serveur EMR via le tunnel ssh<br />\n&emsp;4.9 Connexion au notebook JupyterHub<br />\n&emsp;4.10 Ex\u00e9cution du code<br />\n&emsp;&emsp;4.10.1 D\u00e9marrage de la session Spark<br />\n&emsp;&emsp;4.10.2 Installation des packages<br />\n&emsp;&emsp;4.10.3 Import des librairies<br />\n&emsp;&emsp;4.10.4 D\u00e9finition des PATH pour charger les images et enregistrer les r\u00e9sultats<br />\n&emsp;&emsp;4.10.5 Traitement des donn\u00e9es<br />\n&emsp;&emsp;&emsp;4.10.5.1 Chargement des donn\u00e9es<br />\n&emsp;&emsp;&emsp;4.10.5.2 Pr\u00e9paration du mod\u00e8le<br />\n&emsp;&emsp;&emsp;4.10.5.3 D\u00e9finition du processus de chargement des images<br />\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;et application de leur featurisation \u00e0 travers l'utilisation de pandas UDF<br />\n&emsp;&emsp;&emsp;4.10.5.4 Ex\u00e9cutions des actions d'extractions de features<br />\n&emsp;&emsp;4.10.6 Chargement des donn\u00e9es enregistr\u00e9es et validation du r\u00e9sultat<br />\n&emsp;4.11 Suivi de l'avancement des t\u00e2ches avec le Serveur d'Historique Spark<br />\n&emsp;4.12 R\u00e9siliation de l'instance EMR<br />\n&emsp;4.13 Cloner le serveur EMR (si besoin)<br />\n&emsp;4.14 Arborescence du serveur S3 \u00e0 la fin du projet<br />\n**5. Conclusion**"}, {"metadata": {}, "id": "f251be61", "cell_type": "markdown", "source": "# 1. Pr\u00e9ambule\n\n## 1.1 Probl\u00e9matique\n\nLa tr\u00e8s jeune start-up de l'AgriTech, nomm\u00e9e \"**Fruits**!\", <br />\ncherche \u00e0 proposer des solutions innovantes pour la r\u00e9colte des fruits.\n\nLa volont\u00e9 de l\u2019entreprise est de pr\u00e9server la biodiversit\u00e9 des fruits <br />\nen permettant des traitements sp\u00e9cifiques pour chaque esp\u00e8ce de fruits <br />\nen d\u00e9veloppant des robots cueilleurs intelligents.\n\nLa start-up souhaite dans un premier temps se faire conna\u00eetre en mettant <br />\n\u00e0 disposition du grand public une application mobile qui permettrait aux <br />\nutilisateurs de prendre en photo un fruit et d'obtenir des informations sur ce fruit.\n\nPour la start-up, cette application permettrait de sensibiliser le grand public <br /> \n\u00e0 la biodiversit\u00e9 des fruits et de mettre en place une premi\u00e8re version du moteur <br />\nde classification des images de fruits.\n\nDe plus, le d\u00e9veloppement de l\u2019application mobile permettra de construire <br />\nune premi\u00e8re version de l'architecture **Big Data** n\u00e9cessaire.\n\n## 1.2 Objectifs dans ce projet\n\n1. D\u00e9velopper une premi\u00e8re cha\u00eene de traitement des donn\u00e9es qui <br />\n   comprendra le **preprocessing** et une \u00e9tape de **r\u00e9duction de dimension**.\n2. Tenir compte du fait que <u>le volume de donn\u00e9es va augmenter <br />\n   tr\u00e8s rapidement</u> apr\u00e8s la livraison de ce projet, ce qui implique de:\n - D\u00e9ployer le traitement des donn\u00e9es dans un environnement **Big Data**\n - D\u00e9velopper les scripts en **pyspark** pour effectuer du **calcul distribu\u00e9**"}, {"metadata": {}, "id": "77b554f2", "cell_type": "markdown", "source": "## 1.3 D\u00e9roulement des \u00e9tapes du projet\n\nLe projet va \u00eatre r\u00e9alis\u00e9 en 2 temps, dans deux environnements diff\u00e9rents. <br />\nNous allons dans un premier temps d\u00e9velopper et ex\u00e9cuter notre code en local, <br />\nen travaillant sur un nombre limit\u00e9 d'images \u00e0 traiter.\n\nUne fois les choix techniques valid\u00e9s, nous d\u00e9ploierons notre solution <br />\ndans un environnement Big Data en mode distribu\u00e9.\n\n<u>Pour cette raison, nous allons diviser ce projet en 3 parties</u>:\n1. Liste des choix techniques g\u00e9n\u00e9raux retenus\n2. D\u00e9ploiement de la solution en local\n3. D\u00e9ploiement de la solution dans le cloud"}, {"metadata": {}, "id": "34949cb7", "cell_type": "markdown", "source": "# 2. Choix techniques g\u00e9n\u00e9raux retenus"}, {"metadata": {}, "id": "7936f5af", "cell_type": "markdown", "source": "## 2.1 Calcul distribu\u00e9\n\nL\u2019\u00e9nonc\u00e9 du projet nous conseille de d\u00e9velopper des scripts en **pyspark** <br />\nafin de <u>prendre en compte l\u2019augmentation tr\u00e8s rapide du volume <br />\nde donn\u00e9es apr\u00e8s la livraison du projet</u>.\n\nPour comprendre rapidement et simplement ce qu\u2019est **pyspark** <br />\net son principe de fonctionnement, je vous conseille de lire <br />\ncet article : [PySpark : Tout savoir sur la librairie Python](https://datascientest.com/pyspark)\n\n<u>Le d\u00e9but de l\u2019article nous dit ceci </u>:<br />\n\u00ab *Lorsque l\u2019on parle de traitement de bases de donn\u00e9es sur python, <br />\non pense imm\u00e9diatement \u00e0 la librairie pandas. Cependant, lorsqu\u2019on a <br />\naffaire \u00e0 des bases de donn\u00e9es trop massives, les calculs deviennent trop lents.<br />\nHeureusement, il existe une autre librairie python, assez proche <br />\nde pandas, qui permet de traiter des tr\u00e8s grandes quantit\u00e9s de donn\u00e9es : PySpark*.<br />\n\n*Apache Spark est un framework open-source d\u00e9velopp\u00e9 par l\u2019AMPLab <br />\nde UC Berkeley permettant de traiter des bases de donn\u00e9es massives <br />\nen utilisant le calcul distribu\u00e9, technique qui consiste \u00e0 exploiter <br />\nplusieurs unit\u00e9s de calcul r\u00e9parties en clusters au profit d\u2019un seul <br />\nprojet afin de diviser le temps d\u2019ex\u00e9cution d\u2019une requ\u00eate.*<br />\n\n*Spark a \u00e9t\u00e9 d\u00e9velopp\u00e9 en Scala et est au meilleur de ses capacit\u00e9s <br />\ndans son langage natif. Cependant, la librairie PySpark propose de <br />\nl\u2019utiliser avec le langage Python, en gardant des performances <br />\nsimilaires \u00e0 des impl\u00e9mentations en Scala.<br />\nPyspark est donc une bonne alternative \u00e0 la librairie pandas lorsqu\u2019on <br />\ncherche \u00e0 traiter des jeux de donn\u00e9es trop volumineux qui entra\u00eenent <br />\ndes calculs trop chronophages.* \u00bb\n\nComme nous le constatons, **pySpark** est un moyen de communiquer <br />\navec **Spark** via le langage **Python**.<br />\n**Spark**, quant \u00e0 lui, est un outil qui permet de g\u00e9rer et de coordonner <br />\nl'ex\u00e9cution de t\u00e2ches sur des donn\u00e9es \u00e0 travers un groupe d'ordinateurs. <br />\n\n<u>Spark (ou Apache Spark) est un framework open source de calcul distribu\u00e9 <br />\nin-memory pour le traitement et l'analyse de donn\u00e9es massives</u>.\n\nUn autre [article tr\u00e8s int\u00e9ressant et beaucoup plus complet pour <br />\ncomprendre le **fonctionnement de Spark**](https://www.veonum.com/apache-spark-pour-les-nuls/), ainsi que le r\u00f4le <br />\ndes **Spark Session** que nous utiliserons dans ce projet.\n\n<u>Voici \u00e9galement un extrait</u>:\n\n*Les applications Spark se composent d\u2019un pilote (\u00ab\u202fdriver process\u202f\u00bb) <br />\net de plusieurs ex\u00e9cuteurs (\u00ab\u202fexecutor processes\u202f\u00bb). Il peut \u00eatre configur\u00e9 <br />\npour \u00eatre lui-m\u00eame l\u2019ex\u00e9cuteur (local mode) ou en utiliser autant que <br />\nn\u00e9cessaire pour traiter l\u2019application, Spark prenant en charge la mise <br />\n\u00e0 l\u2019\u00e9chelle automatique par une configuration d\u2019un nombre minimum <br />\net maximum d\u2019ex\u00e9cuteurs.*\n\nLe driver (ou \u00ab\u202fSpark Session\u202f\u00bb) distribue et planifie <br />\nles t\u00e2ches entre les diff\u00e9rents ex\u00e9cuteurs qui r\u00e9alisent les t\u00e2ches <br />\nsur plusieurs serveurs distincts. Autrement dit, il est le responsable <br />\nde l\u2019ex\u00e9cution du code sur les diff\u00e9rentes machines.\n\nChaque ex\u00e9cuteur est un processus Java Virtual Machine (JVM) distinct <br />\ndont il est possible de configurer le nombre de CPU et la quantit\u00e9 de <br />\nm\u00e9moire qui lui est allou\u00e9. <br />\nUne seule t\u00e2che peut traiter un fractionnement de donn\u00e9es \u00e0 la fois.\n\nDans les deux environnements (Local et Cloud) nous utiliserons donc **Spark** <br />\net nous l\u2019exploiterons \u00e0 travers des scripts python gr\u00e2ce \u00e0 **PySpark**.\n\nDans la <u>version locale</u> de notre script nous **simulerons <br />\nle calcul distribu\u00e9** afin de valider que notre solution fonctionne.<br />\nDans la <u>version cloud</u> nous **r\u00e9aliserons les op\u00e9rations sur un cluster de machine**."}, {"metadata": {}, "id": "091b6a61", "cell_type": "markdown", "source": "## 2.2 Transfert Learning\n\nL'\u00e9nonc\u00e9 du projet nous demande \u00e9galement de <br />\nr\u00e9aliser une premi\u00e8re cha\u00eene de traitement <br />\ndes donn\u00e9es qui comprendra le preprocessing et <br />\nune \u00e9tape de r\u00e9duction de dimension.\n\nIl est \u00e9galement pr\u00e9cis\u00e9 qu'il n'est pas n\u00e9cessaire <br />\nd'entra\u00eener un mod\u00e8le pour le moment.\n\nJe d\u00e9cide de partir sur une solution de **[transfert learning](https://keras.io/guides/transfer_learning/)**.\n\nSimplement, le **transfert learning** consiste <br />\n\u00e0 utiliser la connaissance d\u00e9j\u00e0 acquise <br />\npar un mod\u00e8le entra\u00een\u00e9 (ici **MobileNetV2**) pour <br />\nl'adapter \u00e0 notre probl\u00e9matique.\n\nNous allons fournir au mod\u00e8le nos images, et nous allons <br />\n<u>r\u00e9cup\u00e9rer l'avant derni\u00e8re couche</u> du mod\u00e8le.<br />\nEn effet la derni\u00e8re couche de mod\u00e8le est une couche softmax <br />\nqui permet la classification des images ce que nous ne <br />\nsouhaitons pas dans ce projet.\n\nL'avant derni\u00e8re couche correspond \u00e0 un **vecteur <br />\nr\u00e9duit** de dimension (1,1,1280).\n\nCela permettra de r\u00e9aliser une premi\u00e8re version du moteur <br />\npour la classification des images des fruits.\n\n**MobileNetV2** a \u00e9t\u00e9 retenu pour sa <u>rapidit\u00e9 d'ex\u00e9cution</u>, <br />\nparticuli\u00e8rement adapt\u00e9e pour le traitement d'un gros volume <br />\nde donn\u00e9es ainsi que la <u>faible dimensionnalit\u00e9 du vecteur <br />\nde caract\u00e9ristique en sortie</u> (1,1,1280)"}, {"metadata": {}, "id": "43b3cff8", "cell_type": "markdown", "source": "# 3. D\u00e9ploiement de la solution en local\n\n\n## 3.1 Environnement de travail\n\nPour des raisons de simplicit\u00e9, je d\u00e9veloppe dans un environnement <br />\nLinux Unbuntu (ex\u00e9cut\u00e9 depuis une machine MacOS dans une machine virtuelle VirtualBox)\n\n## 3.2 Installation de Spark\n\n[La premi\u00e8re \u00e9tape consiste \u00e0 installer Spark ](https://computingforgeeks.com/how-to-install-apache-spark-on-ubuntu-debian/)\n\n## 3.3 Installation des packages\n\n<u>On installe ensuite \u00e0 l'aide de la commande **pip** <br />\nles packages qui nous seront n\u00e9cessaires</u> :"}, {"metadata": {"scrolled": true, "trusted": true}, "id": "d4100625", "cell_type": "code", "source": "!pip install Pandas pillow tensorflow pyspark pyarrow", "execution_count": 1, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: Pandas in /home/guibr/.local/lib/python3.10/site-packages (1.4.4)\nRequirement already satisfied: pillow in /usr/lib/python3/dist-packages (9.0.1)\nRequirement already satisfied: tensorflow in /home/guibr/.local/lib/python3.10/site-packages (2.10.0)\nRequirement already satisfied: pyspark in /home/guibr/.local/lib/python3.10/site-packages (3.3.0)\nRequirement already satisfied: pyarrow in /home/guibr/.local/lib/python3.10/site-packages (9.0.0)\nRequirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from Pandas) (2022.1)\nRequirement already satisfied: numpy>=1.21.0 in /usr/lib/python3/dist-packages (from Pandas) (1.21.5)\nRequirement already satisfied: python-dateutil>=2.8.1 in /home/guibr/.local/lib/python3.10/site-packages (from Pandas) (2.8.2)\nRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: absl-py>=1.0.0 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (1.2.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (4.3.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (0.27.0)\nRequirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/lib/python3/dist-packages (from tensorflow) (3.12.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: h5py>=2.9.0 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (3.7.0)\nRequirement already satisfied: tensorboard<2.11,>=2.10 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (2.10.0)\nRequirement already satisfied: wrapt>=1.11.0 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (2.10.0)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: keras-preprocessing>=1.1.1 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (1.1.2)\nRequirement already satisfied: astunparse>=1.6.0 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (1.48.1)\nRequirement already satisfied: packaging in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: termcolor>=1.1.0 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (2.0.1)\nRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (14.0.6)\nRequirement already satisfied: flatbuffers>=2.0 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (2.0.7)\nRequirement already satisfied: keras<2.11,>=2.10.0 in /home/guibr/.local/lib/python3.10/site-packages (from tensorflow) (2.10.0)\nRequirement already satisfied: py4j==0.10.9.5 in /home/guibr/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /home/guibr/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.2)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /home/guibr/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.11.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.25.1)\nRequirement already satisfied: markdown>=2.6.8 in /home/guibr/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/guibr/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/guibr/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/guibr/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging->tensorflow) (2.4.7)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /home/guibr/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /home/guibr/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.7.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /home/guibr/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.2.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /home/guibr/.local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /home/guibr/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/guibr/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.0)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"}]}, {"metadata": {}, "id": "5e5f8b46", "cell_type": "markdown", "source": "## 3.4 Import des librairies"}, {"metadata": {"trusted": true}, "id": "79388c4f", "cell_type": "code", "source": "import pandas as pd\nfrom PIL import Image\nimport numpy as np\nimport io\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras import Model\nfrom pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\nfrom pyspark.sql import SparkSession", "execution_count": 2, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2023-03-05 17:35:50.006656: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-03-05 17:35:51.016756: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-03-05 17:35:51.016797: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-03-05 17:35:51.128038: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-03-05 17:35:53.090393: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-03-05 17:35:53.090648: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-03-05 17:35:53.090663: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"}]}, {"metadata": {}, "id": "cabf5df4", "cell_type": "markdown", "source": "## 3.5 D\u00e9finition des PATH pour charger les images <br /> et enregistrer les r\u00e9sultats\n\nDans cette version locale nous partons du principe que les donn\u00e9es <br />\nsont stock\u00e9es dans le m\u00eame r\u00e9pertoire que le notebook.<br />\nNous n'utilisons qu'un \u00e9chantillon de **300 images** \u00e0 traiter dans cette <br />\npremi\u00e8re version en local.<br />\nL'extrait des images \u00e0 charger est stock\u00e9e dans le dossier **test_local_light**.<br />\nNous enregistrerons le r\u00e9sultat de notre traitement <br />\ndans le dossier \"**results_local**\""}, {"metadata": {"scrolled": true, "trusted": true}, "id": "7cbb110a", "cell_type": "code", "source": "PATH = os.getcwd()\nPATH_Data = PATH+'/data/test_local_light'\nPATH_Result = PATH+'/data/results_local'\nprint('PATH:        '+\\\n      PATH+'\\nPATH_Data:   '+\\\n      PATH_Data+'\\nPATH_Result: '+PATH_Result)", "execution_count": 3, "outputs": [{"name": "stdout", "output_type": "stream", "text": "PATH:        /media/sf_shared_folder\nPATH_Data:   /media/sf_shared_folder/data/test_local_light\nPATH_Result: /media/sf_shared_folder/data/results_local\n"}]}, {"metadata": {}, "id": "ce5b85a3", "cell_type": "markdown", "source": "## 3.6 Cr\u00e9ation de la SparkSession\n\nL\u2019application Spark est contr\u00f4l\u00e9e gr\u00e2ce \u00e0 un processus de pilotage (driver process) appel\u00e9 **SparkSession**. <br />\n<u>Une instance de **SparkSession** est la fa\u00e7on dont Spark ex\u00e9cute les fonctions d\u00e9finies par l\u2019utilisateur <br />\ndans l\u2019ensemble du cluster</u>. <u>Une SparkSession correspond toujours \u00e0 une application Spark</u>.\n\n<u>Ici nous cr\u00e9ons une session spark en sp\u00e9cifiant dans l'ordre</u> :\n 1. un **nom pour l'application**, qui sera affich\u00e9e dans l'interface utilisateur Web Spark \"**P8**\"\n 2. que l'application doit s'ex\u00e9cuter **localement**. <br />\n   Nous ne d\u00e9finissons pas le nombre de c\u0153urs \u00e0 utiliser (comme .master('local[4]) pour 4 c\u0153urs \u00e0 utiliser), <br />\n   nous utiliserons donc tous les c\u0153urs disponibles dans notre processeur.<br />\n 3. Que le driver Spark doit pointer vers notre adresse IP (cela \u00e9quivaut \u00e0 la variable d'environnement `SPARK_LOCAL_IP`)\n 4. une option de configuration suppl\u00e9mentaire permettant d'utiliser le **format \"parquet\"** <br />\n   que nous utiliserons pour enregistrer et charger le r\u00e9sultat de notre travail.\n 5. vouloir **obtenir une session spark** existante ou si aucune n'existe, en cr\u00e9er une nouvelle"}, {"metadata": {"trusted": true}, "id": "aace0079", "cell_type": "code", "source": "spark = (SparkSession\n             .builder\n             .appName('P8')\n             .master('local')\n             .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \n             .config(\"spark.sql.parquet.writeLegacyFormat\", 'true')\n             .getOrCreate()\n)", "execution_count": 4, "outputs": [{"name": "stdout", "output_type": "stream", "text": "23/03/05 17:36:01 WARN Utils: Your hostname, Ubuntu22 resolves to a loopback address: 127.0.1.1; using 192.168.1.124 instead (on interface enp0s3)\n23/03/05 17:36:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"}, {"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"}, {"name": "stdout", "output_type": "stream", "text": "23/03/05 17:36:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"}]}, {"metadata": {}, "id": "f69fbdcb", "cell_type": "markdown", "source": "<u>Nous cr\u00e9ons \u00e9galement la variable \"**sc**\" qui est un **SparkContext** issue de la variable **spark**</u> :"}, {"metadata": {"trusted": true}, "id": "dc8c2702", "cell_type": "code", "source": "sc = spark.sparkContext", "execution_count": 5, "outputs": []}, {"metadata": {}, "id": "e9033203", "cell_type": "markdown", "source": "<u>Affichage des informations de Spark en cours d'execution</u> :"}, {"metadata": {"trusted": true}, "id": "2befbda7", "cell_type": "code", "source": "spark", "execution_count": 6, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://192.168.1.124:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local</code></dd>\n              <dt>AppName</dt>\n                <dd><code>P8</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f4576da3a90>"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}]}, {"metadata": {}, "id": "905a9c0f", "cell_type": "markdown", "source": "## 3.7 Traitement des donn\u00e9es\n\n<u>Dans la suite de notre flux de travail, <br />\nnous allons successivement</u> :\n1. Pr\u00e9parer nos donn\u00e9es\n    1. Importer les images dans un dataframe **pandas UDF**\n    2. Associer aux images leur **label**\n    3. Pr\u00e9processer en **redimensionnant nos images pour <br />\n       qu'elles soient compatibles avec notre mod\u00e8le**\n2. Pr\u00e9parer notre mod\u00e8le\n    1. Importer le mod\u00e8le **MobileNetV2**\n    2. Cr\u00e9er un **nouveau mod\u00e8le** d\u00e9pourvu de la derni\u00e8re couche de MobileNetV2\n3. D\u00e9finir le processus de chargement des images et l'application <br />\n   de leur featurisation \u00e0 travers l'utilisation de pandas UDF\n3. Ex\u00e9cuter les actions d'extraction de features\n4. Enregistrer le r\u00e9sultat de nos actions\n5. Tester le bon fonctionnement en chargeant les donn\u00e9es enregistr\u00e9es\n\n\n"}, {"metadata": {}, "id": "0400da14", "cell_type": "markdown", "source": "### 3.7.1 Chargement des donn\u00e9es\n\nLes images sont charg\u00e9es au format binaire, ce qui offre, <br />\nplus de souplesse dans la fa\u00e7on de pr\u00e9traiter les images.\n\nAvant de charger les images, nous sp\u00e9cifions que nous voulons charger <br />\nuniquement les fichiers dont l'extension est **jpg**.\n\nNous indiquons \u00e9galement de charger tous les objets possibles contenus <br />\ndans les sous-dossiers du dossier communiqu\u00e9."}, {"metadata": {"trusted": true}, "id": "c48ecacf", "cell_type": "code", "source": "images = spark.read.format(\"binaryFile\") \\\n  .option(\"pathGlobFilter\", \"*.jpg\") \\\n  .option(\"recursiveFileLookup\", \"true\") \\\n  .load(PATH_Data)", "execution_count": 7, "outputs": []}, {"metadata": {"scrolled": true, "trusted": true}, "id": "5edcf275", "cell_type": "code", "source": "#from PIL import Image\n\n#filepath = PATH_Data + \"/Apple Braeburn/3_100.jpg\"\n\n#with Image.open(filepath) as img:\n#    width, height = img.size\n#    print(width, height)", "execution_count": 8, "outputs": []}, {"metadata": {}, "id": "5770cbbe", "cell_type": "markdown", "source": "<u>Affichage des 5 premi\u00e8res images contenant</u> :\n - le path de l'image\n - la date et heure de sa derni\u00e8re modification\n - sa longueur\n - son contenu encod\u00e9 en valeur hexad\u00e9cimal"}, {"metadata": {}, "id": "d212da34", "cell_type": "markdown", "source": "<u>Je ne conserve que le **path** de l'image et j'ajoute <br />\n    une colonne contenant les **labels** de chaque image</u> :"}, {"metadata": {"trusted": true}, "id": "e9758f53", "cell_type": "code", "source": "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\nprint(images.printSchema())\nprint(images.select('path','label').show(5,False))", "execution_count": 9, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- path: string (nullable = true)\n |-- modificationTime: timestamp (nullable = true)\n |-- length: long (nullable = true)\n |-- content: binary (nullable = true)\n |-- label: string (nullable = true)\n\nNone\n"}, {"name": "stderr", "output_type": "stream", "text": "\r[Stage 0:>                                                          (0 + 1) / 1]\r\r                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+----------------------------------------------------------------------------------+------------------+\n|path                                                                              |label             |\n+----------------------------------------------------------------------------------+------------------+\n|file:/media/sf_shared_folder/data/test_local_light/Apple Crimson Snow/r_25_100.jpg|Apple Crimson Snow|\n|file:/media/sf_shared_folder/data/test_local_light/Apple Crimson Snow/r_24_100.jpg|Apple Crimson Snow|\n|file:/media/sf_shared_folder/data/test_local_light/Apple Crimson Snow/r_30_100.jpg|Apple Crimson Snow|\n|file:/media/sf_shared_folder/data/test_local_light/Apple Crimson Snow/r_32_100.jpg|Apple Crimson Snow|\n|file:/media/sf_shared_folder/data/test_local_light/Apple Crimson Snow/r_26_100.jpg|Apple Crimson Snow|\n+----------------------------------------------------------------------------------+------------------+\nonly showing top 5 rows\n\nNone\n"}]}, {"metadata": {}, "id": "52b85185", "cell_type": "markdown", "source": "### 3.7.2 Pr\u00e9paration du mod\u00e8le\n\nJe vais utiliser la technique du **transfert learning** pour extraire les features des images.<br />\nJ'ai choisi d'utiliser le mod\u00e8le **MobileNetV2** pour sa rapidit\u00e9 d'ex\u00e9cution compar\u00e9e <br />\n\u00e0 d'autres mod\u00e8les comme *VGG16* par exemple.\n\nPour en savoir plus sur la conception et le fonctionnement de MobileNetV2, <br />\nje vous invite \u00e0 lire [cet article](https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c).\n\nIl existe une derni\u00e8re couche qui sert \u00e0 classer les images <br />\nselon 1000 cat\u00e9gories que nous ne voulons pas utiliser.<br />\nL'id\u00e9e dans ce projet est de r\u00e9cup\u00e9rer le **vecteur de caract\u00e9ristiques <br />\nde dimensions (1,1,1280)** qui servira, plus tard, au travers d'un moteur <br />\nde classification \u00e0 reconnaitre les diff\u00e9rents fruits du jeu de donn\u00e9es.\n\nComme d'autres mod\u00e8les similaires, **MobileNetV2**, lorsqu'on l'utilise <br />\nen incluant toutes ses couches, attend obligatoirement des images <br />\nde dimension (224,224,3). Nos images \u00e9tant de dimension (100, 100, 3), <br />\nnous devrons simplement les **redimensionner** avant de les confier au mod\u00e8le.\n\nNous chargeons le mod\u00e8le **MobileNetV2** avec les poids **pr\u00e9calcul\u00e9s** <br />\nissus d'**imagenet** et en sp\u00e9cifiant le format de nos images en entr\u00e9e.<br />\nNous laissons de c\u00f4t\u00e9 la derni\u00e8re couche du mod\u00e8le (`include_top = False`) <br />\nafin de ne pas r\u00e9aliser de classification."}, {"metadata": {"trusted": true}, "id": "00ec1d43", "cell_type": "code", "source": "model = MobileNetV2(input_shape=(224, 224, 3),\n                    weights='imagenet',\n                    include_top=False,\n                    pooling='avg'\n                    )", "execution_count": 10, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2023-03-05 17:36:19.478462: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2023-03-05 17:36:19.479125: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2023-03-05 17:36:19.479164: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Ubuntu22): /proc/driver/nvidia/version does not exist\n2023-03-05 17:36:19.480449: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"}]}, {"metadata": {}, "id": "82b0cf81", "cell_type": "markdown", "source": "Affichage du r\u00e9sum\u00e9 de notre mod\u00e8le o\u00f9 nous constatons <br />\nque <u>nous r\u00e9cup\u00e9rons bien en sortie un vecteur de dimension (1, 1, 1280)</u> :"}, {"metadata": {"trusted": true}, "id": "55384eab", "cell_type": "code", "source": "model.summary()", "execution_count": 11, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Model: \"mobilenetv2_1.00_224\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n                                )]                                                                \n                                                                                                  \n Conv1 (Conv2D)                 (None, 112, 112, 32  864         ['input_1[0][0]']                \n                                )                                                                 \n                                                                                                  \n bn_Conv1 (BatchNormalization)  (None, 112, 112, 32  128         ['Conv1[0][0]']                  \n                                )                                                                 \n                                                                                                  \n Conv1_relu (ReLU)              (None, 112, 112, 32  0           ['bn_Conv1[0][0]']               \n                                )                                                                 \n                                                                                                  \n expanded_conv_depthwise (Depth  (None, 112, 112, 32  288        ['Conv1_relu[0][0]']             \n wiseConv2D)                    )                                                                 \n                                                                                                  \n expanded_conv_depthwise_BN (Ba  (None, 112, 112, 32  128        ['expanded_conv_depthwise[0][0]']\n tchNormalization)              )                                                                 \n                                                                                                  \n expanded_conv_depthwise_relu (  (None, 112, 112, 32  0          ['expanded_conv_depthwise_BN[0][0\n ReLU)                          )                                ]']                              \n                                                                                                  \n expanded_conv_project (Conv2D)  (None, 112, 112, 16  512        ['expanded_conv_depthwise_relu[0]\n                                )                                [0]']                            \n                                                                                                  \n expanded_conv_project_BN (Batc  (None, 112, 112, 16  64         ['expanded_conv_project[0][0]']  \n hNormalization)                )                                                                 \n                                                                                                  \n block_1_expand (Conv2D)        (None, 112, 112, 96  1536        ['expanded_conv_project_BN[0][0]'\n                                )                                ]                                \n                                                                                                  \n block_1_expand_BN (BatchNormal  (None, 112, 112, 96  384        ['block_1_expand[0][0]']         \n ization)                       )                                                                 \n                                                                                                  \n block_1_expand_relu (ReLU)     (None, 112, 112, 96  0           ['block_1_expand_BN[0][0]']      \n                                )                                                                 \n                                                                                                  \n block_1_pad (ZeroPadding2D)    (None, 113, 113, 96  0           ['block_1_expand_relu[0][0]']    \n                                )                                                                 \n                                                                                                  \n block_1_depthwise (DepthwiseCo  (None, 56, 56, 96)  864         ['block_1_pad[0][0]']            \n nv2D)                                                                                            \n                                                                                                  \n block_1_depthwise_BN (BatchNor  (None, 56, 56, 96)  384         ['block_1_depthwise[0][0]']      \n malization)                                                                                      \n                                                                                                  \n block_1_depthwise_relu (ReLU)  (None, 56, 56, 96)   0           ['block_1_depthwise_BN[0][0]']   \n                                                                                                  \n block_1_project (Conv2D)       (None, 56, 56, 24)   2304        ['block_1_depthwise_relu[0][0]'] \n                                                                                                  \n block_1_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_1_project[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block_2_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_1_project_BN[0][0]']     \n                                                                                                  \n block_2_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_2_expand[0][0]']         \n ization)                                                                                         \n                                                                                                  \n block_2_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_2_expand_BN[0][0]']      \n                                                                                                  \n block_2_depthwise (DepthwiseCo  (None, 56, 56, 144)  1296       ['block_2_expand_relu[0][0]']    \n nv2D)                                                                                            \n                                                                                                  \n block_2_depthwise_BN (BatchNor  (None, 56, 56, 144)  576        ['block_2_depthwise[0][0]']      \n malization)                                                                                      \n                                                                                                  \n block_2_depthwise_relu (ReLU)  (None, 56, 56, 144)  0           ['block_2_depthwise_BN[0][0]']   \n                                                                                                  \n block_2_project (Conv2D)       (None, 56, 56, 24)   3456        ['block_2_depthwise_relu[0][0]'] \n                                                                                                  \n block_2_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_2_project[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block_2_add (Add)              (None, 56, 56, 24)   0           ['block_1_project_BN[0][0]',     \n                                                                  'block_2_project_BN[0][0]']     \n                                                                                                  \n block_3_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_2_add[0][0]']            \n                                                                                                  \n block_3_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_3_expand[0][0]']         \n ization)                                                                                         \n                                                                                                  \n"}, {"name": "stdout", "output_type": "stream", "text": " block_3_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_3_expand_BN[0][0]']      \n                                                                                                  \n block_3_pad (ZeroPadding2D)    (None, 57, 57, 144)  0           ['block_3_expand_relu[0][0]']    \n                                                                                                  \n block_3_depthwise (DepthwiseCo  (None, 28, 28, 144)  1296       ['block_3_pad[0][0]']            \n nv2D)                                                                                            \n                                                                                                  \n block_3_depthwise_BN (BatchNor  (None, 28, 28, 144)  576        ['block_3_depthwise[0][0]']      \n malization)                                                                                      \n                                                                                                  \n block_3_depthwise_relu (ReLU)  (None, 28, 28, 144)  0           ['block_3_depthwise_BN[0][0]']   \n                                                                                                  \n block_3_project (Conv2D)       (None, 28, 28, 32)   4608        ['block_3_depthwise_relu[0][0]'] \n                                                                                                  \n block_3_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_3_project[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block_4_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_3_project_BN[0][0]']     \n                                                                                                  \n block_4_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_4_expand[0][0]']         \n ization)                                                                                         \n                                                                                                  \n block_4_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_4_expand_BN[0][0]']      \n                                                                                                  \n block_4_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_4_expand_relu[0][0]']    \n nv2D)                                                                                            \n                                                                                                  \n block_4_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_4_depthwise[0][0]']      \n malization)                                                                                      \n                                                                                                  \n block_4_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_4_depthwise_BN[0][0]']   \n                                                                                                  \n block_4_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_4_depthwise_relu[0][0]'] \n                                                                                                  \n block_4_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_4_project[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block_4_add (Add)              (None, 28, 28, 32)   0           ['block_3_project_BN[0][0]',     \n                                                                  'block_4_project_BN[0][0]']     \n                                                                                                  \n block_5_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_4_add[0][0]']            \n                                                                                                  \n block_5_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_5_expand[0][0]']         \n ization)                                                                                         \n                                                                                                  \n block_5_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_5_expand_BN[0][0]']      \n                                                                                                  \n block_5_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_5_expand_relu[0][0]']    \n nv2D)                                                                                            \n                                                                                                  \n block_5_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_5_depthwise[0][0]']      \n malization)                                                                                      \n                                                                                                  \n block_5_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_5_depthwise_BN[0][0]']   \n                                                                                                  \n block_5_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_5_depthwise_relu[0][0]'] \n                                                                                                  \n block_5_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_5_project[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block_5_add (Add)              (None, 28, 28, 32)   0           ['block_4_add[0][0]',            \n                                                                  'block_5_project_BN[0][0]']     \n                                                                                                  \n block_6_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_5_add[0][0]']            \n                                                                                                  \n block_6_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_6_expand[0][0]']         \n ization)                                                                                         \n                                                                                                  \n block_6_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_6_expand_BN[0][0]']      \n                                                                                                  \n block_6_pad (ZeroPadding2D)    (None, 29, 29, 192)  0           ['block_6_expand_relu[0][0]']    \n                                                                                                  \n block_6_depthwise (DepthwiseCo  (None, 14, 14, 192)  1728       ['block_6_pad[0][0]']            \n nv2D)                                                                                            \n                                                                                                  \n block_6_depthwise_BN (BatchNor  (None, 14, 14, 192)  768        ['block_6_depthwise[0][0]']      \n malization)                                                                                      \n                                                                                                  \n block_6_depthwise_relu (ReLU)  (None, 14, 14, 192)  0           ['block_6_depthwise_BN[0][0]']   \n                                                                                                  \n block_6_project (Conv2D)       (None, 14, 14, 64)   12288       ['block_6_depthwise_relu[0][0]'] \n                                                                                                  \n block_6_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_6_project[0][0]']        \n"}, {"name": "stdout", "output_type": "stream", "text": " lization)                                                                                        \n                                                                                                  \n block_7_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_6_project_BN[0][0]']     \n                                                                                                  \n block_7_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_7_expand[0][0]']         \n ization)                                                                                         \n                                                                                                  \n block_7_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_7_expand_BN[0][0]']      \n                                                                                                  \n block_7_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_7_expand_relu[0][0]']    \n nv2D)                                                                                            \n                                                                                                  \n block_7_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_7_depthwise[0][0]']      \n malization)                                                                                      \n                                                                                                  \n block_7_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_7_depthwise_BN[0][0]']   \n                                                                                                  \n block_7_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_7_depthwise_relu[0][0]'] \n                                                                                                  \n block_7_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_7_project[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block_7_add (Add)              (None, 14, 14, 64)   0           ['block_6_project_BN[0][0]',     \n                                                                  'block_7_project_BN[0][0]']     \n                                                                                                  \n block_8_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_7_add[0][0]']            \n                                                                                                  \n block_8_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_8_expand[0][0]']         \n ization)                                                                                         \n                                                                                                  \n block_8_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_8_expand_BN[0][0]']      \n                                                                                                  \n block_8_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_8_expand_relu[0][0]']    \n nv2D)                                                                                            \n                                                                                                  \n block_8_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_8_depthwise[0][0]']      \n malization)                                                                                      \n                                                                                                  \n block_8_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_8_depthwise_BN[0][0]']   \n                                                                                                  \n block_8_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_8_depthwise_relu[0][0]'] \n                                                                                                  \n block_8_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_8_project[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block_8_add (Add)              (None, 14, 14, 64)   0           ['block_7_add[0][0]',            \n                                                                  'block_8_project_BN[0][0]']     \n                                                                                                  \n block_9_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_8_add[0][0]']            \n                                                                                                  \n block_9_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_9_expand[0][0]']         \n ization)                                                                                         \n                                                                                                  \n block_9_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_9_expand_BN[0][0]']      \n                                                                                                  \n block_9_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_9_expand_relu[0][0]']    \n nv2D)                                                                                            \n                                                                                                  \n block_9_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_9_depthwise[0][0]']      \n malization)                                                                                      \n                                                                                                  \n block_9_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_9_depthwise_BN[0][0]']   \n                                                                                                  \n block_9_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_9_depthwise_relu[0][0]'] \n                                                                                                  \n block_9_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_9_project[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block_9_add (Add)              (None, 14, 14, 64)   0           ['block_8_add[0][0]',            \n                                                                  'block_9_project_BN[0][0]']     \n                                                                                                  \n block_10_expand (Conv2D)       (None, 14, 14, 384)  24576       ['block_9_add[0][0]']            \n                                                                                                  \n block_10_expand_BN (BatchNorma  (None, 14, 14, 384)  1536       ['block_10_expand[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block_10_expand_relu (ReLU)    (None, 14, 14, 384)  0           ['block_10_expand_BN[0][0]']     \n                                                                                                  \n block_10_depthwise (DepthwiseC  (None, 14, 14, 384)  3456       ['block_10_expand_relu[0][0]']   \n onv2D)                                                                                           \n                                                                                                  \n block_10_depthwise_BN (BatchNo  (None, 14, 14, 384)  1536       ['block_10_depthwise[0][0]']     \n rmalization)                                                                                     \n"}, {"name": "stdout", "output_type": "stream", "text": "                                                                                                  \n block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0          ['block_10_depthwise_BN[0][0]']  \n                                                                                                  \n block_10_project (Conv2D)      (None, 14, 14, 96)   36864       ['block_10_depthwise_relu[0][0]']\n                                                                                                  \n block_10_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_10_project[0][0]']       \n alization)                                                                                       \n                                                                                                  \n block_11_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_10_project_BN[0][0]']    \n                                                                                                  \n block_11_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_11_expand[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block_11_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_11_expand_BN[0][0]']     \n                                                                                                  \n block_11_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_11_expand_relu[0][0]']   \n onv2D)                                                                                           \n                                                                                                  \n block_11_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_11_depthwise[0][0]']     \n rmalization)                                                                                     \n                                                                                                  \n block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_11_depthwise_BN[0][0]']  \n                                                                                                  \n block_11_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_11_depthwise_relu[0][0]']\n                                                                                                  \n block_11_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_11_project[0][0]']       \n alization)                                                                                       \n                                                                                                  \n block_11_add (Add)             (None, 14, 14, 96)   0           ['block_10_project_BN[0][0]',    \n                                                                  'block_11_project_BN[0][0]']    \n                                                                                                  \n block_12_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_11_add[0][0]']           \n                                                                                                  \n block_12_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_12_expand[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block_12_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_12_expand_BN[0][0]']     \n                                                                                                  \n block_12_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_12_expand_relu[0][0]']   \n onv2D)                                                                                           \n                                                                                                  \n block_12_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_12_depthwise[0][0]']     \n rmalization)                                                                                     \n                                                                                                  \n block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_12_depthwise_BN[0][0]']  \n                                                                                                  \n block_12_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_12_depthwise_relu[0][0]']\n                                                                                                  \n block_12_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_12_project[0][0]']       \n alization)                                                                                       \n                                                                                                  \n block_12_add (Add)             (None, 14, 14, 96)   0           ['block_11_add[0][0]',           \n                                                                  'block_12_project_BN[0][0]']    \n                                                                                                  \n block_13_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_12_add[0][0]']           \n                                                                                                  \n block_13_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_13_expand[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block_13_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_13_expand_BN[0][0]']     \n                                                                                                  \n block_13_pad (ZeroPadding2D)   (None, 15, 15, 576)  0           ['block_13_expand_relu[0][0]']   \n                                                                                                  \n block_13_depthwise (DepthwiseC  (None, 7, 7, 576)   5184        ['block_13_pad[0][0]']           \n onv2D)                                                                                           \n                                                                                                  \n block_13_depthwise_BN (BatchNo  (None, 7, 7, 576)   2304        ['block_13_depthwise[0][0]']     \n rmalization)                                                                                     \n                                                                                                  \n block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)   0           ['block_13_depthwise_BN[0][0]']  \n                                                                                                  \n block_13_project (Conv2D)      (None, 7, 7, 160)    92160       ['block_13_depthwise_relu[0][0]']\n                                                                                                  \n block_13_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_13_project[0][0]']       \n alization)                                                                                       \n                                                                                                  \n block_14_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_13_project_BN[0][0]']    \n                                                                                                  \n block_14_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_14_expand[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block_14_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_14_expand_BN[0][0]']     \n                                                                                                  \n"}, {"name": "stdout", "output_type": "stream", "text": " block_14_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_14_expand_relu[0][0]']   \n onv2D)                                                                                           \n                                                                                                  \n block_14_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_14_depthwise[0][0]']     \n rmalization)                                                                                     \n                                                                                                  \n block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_14_depthwise_BN[0][0]']  \n                                                                                                  \n block_14_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_14_depthwise_relu[0][0]']\n                                                                                                  \n block_14_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_14_project[0][0]']       \n alization)                                                                                       \n                                                                                                  \n block_14_add (Add)             (None, 7, 7, 160)    0           ['block_13_project_BN[0][0]',    \n                                                                  'block_14_project_BN[0][0]']    \n                                                                                                  \n block_15_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_14_add[0][0]']           \n                                                                                                  \n block_15_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_15_expand[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block_15_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_15_expand_BN[0][0]']     \n                                                                                                  \n block_15_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_15_expand_relu[0][0]']   \n onv2D)                                                                                           \n                                                                                                  \n block_15_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_15_depthwise[0][0]']     \n rmalization)                                                                                     \n                                                                                                  \n block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_15_depthwise_BN[0][0]']  \n                                                                                                  \n block_15_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_15_depthwise_relu[0][0]']\n                                                                                                  \n block_15_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_15_project[0][0]']       \n alization)                                                                                       \n                                                                                                  \n block_15_add (Add)             (None, 7, 7, 160)    0           ['block_14_add[0][0]',           \n                                                                  'block_15_project_BN[0][0]']    \n                                                                                                  \n block_16_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_15_add[0][0]']           \n                                                                                                  \n block_16_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_16_expand[0][0]']        \n lization)                                                                                        \n                                                                                                  \n block_16_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_16_expand_BN[0][0]']     \n                                                                                                  \n block_16_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_16_expand_relu[0][0]']   \n onv2D)                                                                                           \n                                                                                                  \n block_16_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_16_depthwise[0][0]']     \n rmalization)                                                                                     \n                                                                                                  \n block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_16_depthwise_BN[0][0]']  \n                                                                                                  \n block_16_project (Conv2D)      (None, 7, 7, 320)    307200      ['block_16_depthwise_relu[0][0]']\n                                                                                                  \n block_16_project_BN (BatchNorm  (None, 7, 7, 320)   1280        ['block_16_project[0][0]']       \n alization)                                                                                       \n                                                                                                  \n Conv_1 (Conv2D)                (None, 7, 7, 1280)   409600      ['block_16_project_BN[0][0]']    \n                                                                                                  \n Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)  5120        ['Conv_1[0][0]']                 \n                                                                                                  \n out_relu (ReLU)                (None, 7, 7, 1280)   0           ['Conv_1_bn[0][0]']              \n                                                                                                  \n global_average_pooling2d (Glob  (None, 1280)        0           ['out_relu[0][0]']               \n alAveragePooling2D)                                                                              \n                                                                                                  \n==================================================================================================\nTotal params: 2,257,984\nTrainable params: 2,223,872\nNon-trainable params: 34,112\n__________________________________________________________________________________________________\n"}]}, {"metadata": {}, "id": "e8fcd559", "cell_type": "markdown", "source": "Tous les workeurs doivent pouvoir acc\u00e9der au mod\u00e8le ainsi qu'\u00e0 ses poids. <br />\nUne bonne pratique consiste \u00e0 charger le mod\u00e8le sur le driver puis \u00e0 diffuser <br />\nensuite les poids aux diff\u00e9rents workeurs."}, {"metadata": {"trusted": true}, "id": "3b2ff766", "cell_type": "code", "source": "brodcast_weights = sc.broadcast(model.get_weights())", "execution_count": 12, "outputs": []}, {"metadata": {}, "id": "c0c47970", "cell_type": "markdown", "source": "<u>Mettons cela sous forme de fonction</u> :"}, {"metadata": {"trusted": true}, "id": "cae9e323", "cell_type": "code", "source": "def model_fn():\n    \"\"\"\n    Returns a MobileNetV2 model with top layer removed \n    and broadcasted pretrained weights.\n    \"\"\"\n    model = MobileNetV2(weights='imagenet',\n                        include_top=False,\n                        input_shape=(224, 224, 3),\n                        pooling='avg')\n    for layer in model.layers:\n        layer.trainable = False\n    model.set_weights(brodcast_weights.value)\n    return model", "execution_count": 13, "outputs": []}, {"metadata": {}, "id": "d58b940d", "cell_type": "markdown", "source": "### 3.7.3 D\u00e9finition du processus de chargement des images et application <br/>de leur featurisation \u00e0 travers l'utilisation de pandas UDF\n\nCe notebook d\u00e9finit la logique par \u00e9tapes, jusqu'\u00e0 Pandas UDF.\n\n<u>L'empilement des appels est la suivante</u> :\n\n- Pandas UDF\n  - featuriser une s\u00e9rie d'images pd.Series\n   - pr\u00e9traiter une image"}, {"metadata": {"trusted": true}, "id": "bc4a9885", "cell_type": "code", "source": "def preprocess(content):\n    \"\"\"\n    Preprocesses raw image bytes for prediction.\n    \"\"\"\n    img = Image.open(io.BytesIO(content)).resize([224, 224])\n    arr = img_to_array(img)\n    return preprocess_input(arr)\n\ndef featurize_series(model, content_series):\n    \"\"\"\n    Featurize a pd.Series of raw images using the input model.\n    :return: a pd.Series of image features\n    \"\"\"\n    input = np.stack(content_series.map(preprocess))\n    preds = model.predict(input)\n    # For some layers, output features will be multi-dimensional tensors.\n    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n    output = [p.flatten() for p in preds]\n    return pd.Series(output)\n\n@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\ndef featurize_udf(content_series_iter):\n    '''\n    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n\n    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n                              is a pandas Series of image data.\n    '''\n    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n    # for multiple data batches.  This amortizes the overhead of loading big models.\n    model = model_fn()\n    for content_series in content_series_iter:\n        yield featurize_series(model, content_series)", "execution_count": 14, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/home/guibr/.local/lib/python3.10/site-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n  warnings.warn(\n"}]}, {"metadata": {}, "id": "3abbc12e", "cell_type": "markdown", "source": "### 3.7.4 Ex\u00e9cution des actions d'extraction de features\n\nLes Pandas UDF, sur de grands enregistrements (par exemple, de tr\u00e8s grandes images), <br />\npeuvent rencontrer des erreurs de type Out Of Memory (OOM).<br />\nSi vous rencontrez de telles erreurs dans la cellule ci-dessous, <br />\nessayez de r\u00e9duire la taille du lot Arrow via 'maxRecordsPerBatch'\n\nJe n'utiliserai pas cette commande dans ce projet <br />\net je laisse donc la commande en commentaire."}, {"metadata": {"trusted": true}, "id": "8a9bcf8d", "cell_type": "code", "source": "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")", "execution_count": 15, "outputs": []}, {"metadata": {}, "id": "078aa0f6", "cell_type": "markdown", "source": "Nous pouvons maintenant ex\u00e9cuter la featurisation sur l'ensemble de notre DataFrame Spark.<br />\n<u>REMARQUE</u> : Cela peut prendre beaucoup de temps, tout d\u00e9pend du volume de donn\u00e9es \u00e0 traiter. <br />\n\nNotre jeu de donn\u00e9es de **Test** contient **22 819 images**. <br /> \nCependant, dans l'ex\u00e9cution en mode **local**, <br /> \nnous <u>traiterons un ensemble r\u00e9duit de **300 images**</u>. "}, {"metadata": {"trusted": true}, "id": "334badb0", "cell_type": "code", "source": "features_df = images.repartition(20).select(col(\"path\"),\n                                            col(\"label\"),\n                                            featurize_udf(\"content\").alias(\"features\")\n                                           )", "execution_count": 16, "outputs": []}, {"metadata": {"trusted": true}, "id": "b4a37843", "cell_type": "code", "source": "print(features_df.count(), len(features_df.columns))", "execution_count": 17, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1:====================================================>     (9 + 1) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "312 3\n"}, {"name": "stderr", "output_type": "stream", "text": "\r[Stage 3:===================================================>     (18 + 1) / 20]\r\r                                                                                \r"}]}, {"metadata": {"scrolled": true, "trusted": true}, "id": "09e72475", "cell_type": "code", "source": "features_df.show()", "execution_count": 18, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2023-03-05 17:36:33.689226: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-03-05 17:36:33.877040: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-03-05 17:36:33.877269: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-03-05 17:36:33.922442: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-03-05 17:36:35.696510: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-03-05 17:36:35.697138: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-03-05 17:36:35.697604: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n2023-03-05 17:36:37.641599: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2023-03-05 17:36:37.642113: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2023-03-05 17:36:37.642225: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Ubuntu22): /proc/driver/nvidia/version does not exist\n2023-03-05 17:36:37.642508: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-03-05 17:36:40.453502: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 22478848 exceeds 10% of free system memory.\n2023-03-05 17:36:40.564619: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 22478848 exceeds 10% of free system memory.\n2023-03-05 17:36:40.595154: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 22478848 exceeds 10% of free system memory.\n2023-03-05 17:36:40.621927: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 11239424 exceeds 10% of free system memory.\n2023-03-05 17:36:40.634078: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 67436544 exceeds 10% of free system memory.\n1/1 [==============================] - 1s 1s/step\n1/1 [==============================] - 1s 1s/step                   (0 + 1) / 1]\n"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+------------------+--------------------+\n|                path|             label|            features|\n+--------------------+------------------+--------------------+\n|file:/media/sf_sh...|Apple Crimson Snow|[0.0, 0.0, 0.0, 0...|\n|file:/media/sf_sh...|    Apple Braeburn|[0.0, 0.009954287...|\n|file:/media/sf_sh...|    Apple Braeburn|[0.61726123, 0.05...|\n|file:/media/sf_sh...|Apple Crimson Snow|[0.03528473, 0.0,...|\n|file:/media/sf_sh...|Apple Crimson Snow|[0.0, 0.0, 0.0, 0...|\n|file:/media/sf_sh...|Apple Crimson Snow|[0.016255837, 0.0...|\n|file:/media/sf_sh...|Apple Crimson Snow|[0.017087255, 0.0...|\n|file:/media/sf_sh...|Apple Crimson Snow|[0.2577594, 0.0, ...|\n|file:/media/sf_sh...|Apple Crimson Snow|[7.6287135E-4, 0....|\n|file:/media/sf_sh...|Apple Crimson Snow|[0.44335353, 0.0,...|\n|file:/media/sf_sh...|    Apple Braeburn|[0.7459447, 0.021...|\n|file:/media/sf_sh...|    Apple Braeburn|[0.53476775, 0.00...|\n|file:/media/sf_sh...|    Apple Braeburn|[0.7728701, 0.103...|\n|file:/media/sf_sh...|    Apple Braeburn|[0.46679166, 0.20...|\n|file:/media/sf_sh...|Apple Crimson Snow|[0.0, 0.0, 0.0, 0...|\n|file:/media/sf_sh...|Apple Crimson Snow|[0.0, 0.0, 0.0, 0...|\n|file:/media/sf_sh...|Apple Crimson Snow|[0.0, 0.018437035...|\n|file:/media/sf_sh...|Apple Crimson Snow|[0.006971087, 0.0...|\n|file:/media/sf_sh...|Apple Crimson Snow|[0.04521227, 0.0,...|\n|file:/media/sf_sh...|Apple Crimson Snow|[0.0, 0.023632906...|\n+--------------------+------------------+--------------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "\r                                                                                \r"}]}, {"metadata": {}, "id": "81a01ed4", "cell_type": "markdown", "source": "### 3.7.5 R\u00e9duction de dimension (PCA)\n\nPour r\u00e9aliser une r\u00e9duction de dimension via Spark nous allons entrainer </br> \nun **mod\u00e8le de PCA** disponible dans le module ml de pyspark. </br>\n\n"}, {"metadata": {"scrolled": true, "trusted": true}, "id": "255edda8", "cell_type": "code", "source": "features_df.printSchema()", "execution_count": 19, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- path: string (nullable = true)\n |-- label: string (nullable = true)\n |-- features: array (nullable = true)\n |    |-- element: float (containsNull = true)\n\n"}]}, {"metadata": {"scrolled": false, "trusted": true}, "id": "dfbfd28e", "cell_type": "code", "source": "from pyspark.ml.feature import PCA\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\n\n# Define a UDF to convert the features column to a list\nto_list_udf = udf(lambda row: [float(i) for i in row], ArrayType(FloatType()))\n\n# Convert the features column to a list and create a new column \"features_list\"\nfeatures_df = features_df.withColumn(\"features_list\", to_list_udf(features_df[\"features\"]))\n\n# Define UDF to convert list to DenseVector\nto_dense_vector_udf = udf(lambda x: Vectors.dense(x), VectorUDT())\n\n# Convert the \"features_list\" column to DenseVector and create a new column \"features_vec\"\nfeatures_df = features_df.withColumn(\"features_vec\", to_dense_vector_udf(features_df[\"features_list\"]))\n\n# Drop the \"features_list\" column\nfeatures_df = features_df.drop(\"features_list\")\n\n# Print the updated schema\nfeatures_df.printSchema()", "execution_count": 20, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- path: string (nullable = true)\n |-- label: string (nullable = true)\n |-- features: array (nullable = true)\n |    |-- element: float (containsNull = true)\n |-- features_vec: vector (nullable = true)\n\n"}]}, {"metadata": {"trusted": true}, "id": "0750a819", "cell_type": "code", "source": "scaler = StandardScaler(\n    inputCol = 'features_vec', \n    outputCol = 'scaledFeatures',\n    withMean = True,\n    withStd = True\n).fit(features_df)\n\n# when we transform the dataframe, the old\n# feature will still remain in it\nfeatures_df = scaler.transform(features_df)\nfeatures_df.show(6)", "execution_count": 21, "outputs": [{"name": "stderr", "output_type": "stream", "text": "1/1 [==============================] - 2s 2s/step                  (0 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (1 + 1) / 20]\n1/1 [==============================] - ETA: 0sWARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f20c1a52d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n1/1 [==============================] - 1s 1s/step\n1/1 [==============================] - ETA: 0sWARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f21139e8430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 2s 2s/step                  (4 + 1) / 20]\n1/1 [==============================] - 2s 2s/step                  (5 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (6 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (7 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (8 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (9 + 1) / 20]\n1/1 [==============================] - 2s 2s/step                 (10 + 1) / 20]\n1/1 [==============================] - 2s 2s/step                 (11 + 1) / 20]\n1/1 [==============================] - 2s 2s/step                 (12 + 1) / 20]\n1/1 [==============================] - 2s 2s/step                 (13 + 1) / 20]\n1/1 [==============================] - 1s 1s/step>                (14 + 1) / 20]\n1/1 [==============================] - 3s 3s/step===>             (15 + 1) / 20]\n1/1 [==============================] - 1s 1s/step=====>           (16 + 1) / 20]\n1/1 [==============================] - 1s 1s/step========>        (17 + 1) / 20]\n1/1 [==============================] - 1s 1s/step===========>     (18 + 1) / 20]\n1/1 [==============================] - 1s 1s/step==============>  (19 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                   (0 + 1) / 1]\n"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+------------------+--------------------+--------------------+--------------------+\n|                path|             label|            features|        features_vec|      scaledFeatures|\n+--------------------+------------------+--------------------+--------------------+--------------------+\n|file:/media/sf_sh...|Apple Crimson Snow|[0.0, 0.0, 0.0, 0...|[0.0,0.0,0.0,0.0,...|[-0.9004491487914...|\n|file:/media/sf_sh...|    Apple Braeburn|[0.0, 0.009954287...|[0.0,0.0099542867...|[-0.9004491487914...|\n|file:/media/sf_sh...|    Apple Braeburn|[0.61726123, 0.05...|[0.61726123094558...|[0.52119492182217...|\n|file:/media/sf_sh...|Apple Crimson Snow|[0.03528473, 0.0,...|[0.03528473153710...|[-0.8191831841429...|\n|file:/media/sf_sh...|Apple Crimson Snow|[0.0, 0.0, 0.0, 0...|[0.0,0.0,0.0,0.0,...|[-0.9004491487914...|\n|file:/media/sf_sh...|Apple Crimson Snow|[0.016255837, 0.0...|[0.01625583693385...|[-0.8630095478068...|\n+--------------------+------------------+--------------------+--------------------+--------------------+\nonly showing top 6 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "\r                                                                                \r"}]}, {"metadata": {"trusted": true}, "id": "675ab2a7", "cell_type": "code", "source": "#first_record = df_scaled.collect()[0]['scaledFeatures']", "execution_count": 72, "outputs": [{"name": "stderr", "output_type": "stream", "text": "1/1 [==============================] - 2s 2s/step                  (0 + 1) / 20]\n1/1 [==============================] - 2s 2s/step                  (1 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (2 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (3 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (4 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (5 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (6 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (7 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (8 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (9 + 1) / 20]\n1/1 [==============================] - 2s 2s/step                 (10 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                 (11 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                 (12 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                 (13 + 1) / 20]\n1/1 [==============================] - 2s 2s/step>                (14 + 1) / 20]\n1/1 [==============================] - 1s 1s/step===>             (15 + 1) / 20]\n1/1 [==============================] - 2s 2s/step=====>           (16 + 1) / 20]\n1/1 [==============================] - 1s 1s/step========>        (17 + 1) / 20]\n1/1 [==============================] - 1s 1s/step===========>     (18 + 1) / 20]\n1/1 [==============================] - 2s 2s/step==============>  (19 + 1) / 20]\n                                                                                \r"}]}, {"metadata": {"trusted": true}, "id": "23dd050d", "cell_type": "code", "source": "#first_record.size", "execution_count": 75, "outputs": [{"data": {"text/plain": "1280"}, "execution_count": 75, "metadata": {}, "output_type": "execute_result"}]}, {"metadata": {"scrolled": true, "trusted": true}, "id": "8c4b0a2c", "cell_type": "code", "source": "n_components = 2\npca = PCA(\n    k = n_components, \n    inputCol = 'scaledFeatures', \n    outputCol = 'pcaFeatures'\n).fit(df_scaled)", "execution_count": 22, "outputs": [{"name": "stderr", "output_type": "stream", "text": "1/1 [==============================] - 1s 1000ms/step               (0 + 1) / 1]\n1/1 [==============================] - 1s 938ms/step                (0 + 1) / 1]\n1/1 [==============================] - 1s 932ms/step               (0 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (1 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (2 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (3 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (4 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (5 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (6 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (7 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (8 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (9 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                 (10 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                 (11 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                 (12 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                 (13 + 1) / 20]\n1/1 [==============================] - 1s 1s/step>                (14 + 1) / 20]\n1/1 [==============================] - 1s 1s/step===>             (15 + 1) / 20]\n1/1 [==============================] - 1s 1s/step=====>           (16 + 1) / 20]\n1/1 [==============================] - 1s 1s/step========>        (17 + 1) / 20]\n1/1 [==============================] - 2s 2s/step===========>     (18 + 1) / 20]\n1/1 [==============================] - 1s 1s/step==============>  (19 + 1) / 20]\n1/1 [==============================] - 1s 975ms/step                (0 + 1) / 1]\n1/1 [==============================] - 1s 932ms/step               (0 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (1 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (2 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (3 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (4 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (5 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (6 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (7 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (8 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                  (9 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                 (10 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                 (11 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                 (12 + 1) / 20]\n1/1 [==============================] - 1s 1s/step                 (13 + 1) / 20]\n1/1 [==============================] - 1s 1s/step>                (14 + 1) / 20]\n1/1 [==============================] - 1s 1s/step===>             (15 + 1) / 20]\n1/1 [==============================] - 1s 1s/step=====>           (16 + 1) / 20]\n1/1 [==============================] - 1s 1s/step========>        (17 + 1) / 20]\n1/1 [==============================] - 1s 1s/step===========>     (18 + 1) / 20]\n1/1 [==============================] - 1s 1s/step==============>  (19 + 1) / 20]\n"}, {"name": "stdout", "output_type": "stream", "text": "23/03/05 17:41:04 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks \njava.io.IOException: Failed to connect to /192.168.1.124:35869\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:102)\n\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1144)\n\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1088)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1088)\n\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1228)\n\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connexion refus\u00e9e: /192.168.1.124:35869\nCaused by: java.net.ConnectException: Connexion refus\u00e9e\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:710)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"}, {"name": "stderr", "output_type": "stream", "text": "\r[Stage 33:>                                                         (0 + 2) / 4]\r\r[Stage 33:>                                                         (0 + 3) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "23/03/05 17:41:04 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks \njava.io.IOException: Connecting to /192.168.1.124:35869 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:102)\n\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1144)\n\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1088)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1088)\n\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1228)\n\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/03/05 17:41:04 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks \njava.io.IOException: Connecting to /192.168.1.124:35869 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:102)\n\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1144)\n\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1088)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1088)\n\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1228)\n\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/03/05 17:41:04 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks \njava.io.IOException: Connecting to /192.168.1.124:35869 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:102)\n\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1144)\n\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1088)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1088)\n\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1228)\n\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"}, {"name": "stderr", "output_type": "stream", "text": "\r[Stage 33:>                                                         (0 + 4) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "23/03/05 17:41:09 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 1 retries)\njava.io.IOException: Failed to connect to /192.168.1.124:35869\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connexion refus\u00e9e: /192.168.1.124:35869\nCaused by: java.net.ConnectException: Connexion refus\u00e9e\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:710)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/03/05 17:41:09 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 1 retries)\njava.io.IOException: Connecting to /192.168.1.124:35869 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/03/05 17:41:09 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 1 retries)\njava.io.IOException: Connecting to /192.168.1.124:35869 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/03/05 17:41:09 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 1 retries)\njava.io.IOException: Connecting to /192.168.1.124:35869 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/03/05 17:41:14 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 2 retries)\njava.io.IOException: Failed to connect to /192.168.1.124:35869\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connexion refus\u00e9e: /192.168.1.124:35869\nCaused by: java.net.ConnectException: Connexion refus\u00e9e\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:710)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"}, {"name": "stdout", "output_type": "stream", "text": "23/03/05 17:41:14 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 2 retries)\njava.io.IOException: Connecting to /192.168.1.124:35869 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/03/05 17:41:14 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 2 retries)\njava.io.IOException: Connecting to /192.168.1.124:35869 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/03/05 17:41:14 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 2 retries)\njava.io.IOException: Connecting to /192.168.1.124:35869 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/03/05 17:41:19 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 3 retries)\njava.io.IOException: Failed to connect to /192.168.1.124:35869\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connexion refus\u00e9e: /192.168.1.124:35869\nCaused by: java.net.ConnectException: Connexion refus\u00e9e\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:710)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/03/05 17:41:19 WARN BlockManager: Failed to fetch remote block taskresult_143 from [BlockManagerId(driver, 192.168.1.124, 35869, None)] after 1 fetch failures. Most recent failure cause:\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:103)\n\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1144)\n\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1088)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1088)\n\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1228)\n\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.124:35869\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connexion refus\u00e9e: /192.168.1.124:35869\nCaused by: java.net.ConnectException: Connexion refus\u00e9e\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:710)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/03/05 17:41:19 WARN TaskSetManager: Lost task 0.0 in stage 33.0 (TID 143) (192.168.1.124 executor driver): TaskResultLost (result lost from block manager)\n23/03/05 17:41:19 ERROR TaskSetManager: Task 0 in stage 33.0 failed 1 times; aborting job\n"}, {"name": "stderr", "output_type": "stream", "text": "\r[Stage 33:>                                                         (0 + 3) / 4]\r\r[Stage 33:>                                                         (0 + 2) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "23/03/05 17:41:19 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 3 retries)\njava.io.IOException: Connecting to /192.168.1.124:35869 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/03/05 17:41:19 WARN BlockManager: Failed to fetch remote block taskresult_144 from [BlockManagerId(driver, 192.168.1.124, 35869, None)] after 1 fetch failures. Most recent failure cause:\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:103)\n\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1144)\n\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1088)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1088)\n\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1228)\n\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Connecting to /192.168.1.124:35869 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n23/03/05 17:41:19 WARN TaskSetManager: Lost task 1.0 in stage 33.0 (TID 144) (192.168.1.124 executor driver): TaskResultLost (result lost from block manager)\n23/03/05 17:41:19 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 3 retries)\njava.io.IOException: Connecting to /192.168.1.124:35869 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/03/05 17:41:19 WARN BlockManager: Failed to fetch remote block taskresult_145 from [BlockManagerId(driver, 192.168.1.124, 35869, None)] after 1 fetch failures. Most recent failure cause:\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:103)\n\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1144)\n\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1088)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1088)\n\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1228)\n\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Connecting to /192.168.1.124:35869 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n23/03/05 17:41:19 WARN TaskSetManager: Lost task 2.0 in stage 33.0 (TID 145) (192.168.1.124 executor driver): TaskResultLost (result lost from block manager)\n23/03/05 17:41:19 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 3 retries)\njava.io.IOException: Connecting to /192.168.1.124:35869 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/03/05 17:41:19 WARN BlockManager: Failed to fetch remote block taskresult_146 from [BlockManagerId(driver, 192.168.1.124, 35869, None)] after 1 fetch failures. Most recent failure cause:\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:103)\n\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1144)\n\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1088)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1088)\n\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1228)\n\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Connecting to /192.168.1.124:35869 failed in the last 4750 ms, fail this connection directly\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n23/03/05 17:41:19 WARN TaskSetManager: Lost task 3.0 in stage 33.0 (TID 146) (192.168.1.124 executor driver): TaskResultLost (result lost from block manager)\n"}, {"ename": "Py4JJavaError", "evalue": "An error occurred while calling o161.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 143) (192.168.1.124 executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeDenseVectorCovariance(RowMatrix.scala:171)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:463)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:499)\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:65)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:93)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "Cell \u001b[0;32mIn [22], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m n_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      2\u001b[0m pca \u001b[38;5;241m=\u001b[39m \u001b[43mPCA\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputCol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscaledFeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputCol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpcaFeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m----> 6\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_scaled\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n", "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:379\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 379\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n", "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:376\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n", "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n", "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n", "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o161.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 143) (192.168.1.124 executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeDenseVectorCovariance(RowMatrix.scala:171)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:463)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:499)\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:65)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:93)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"]}, {"name": "stdout", "output_type": "stream", "text": "23/03/07 12:05:43 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 152025830 ms exceeds timeout 120000 ms\n23/03/07 12:05:43 WARN SparkContext: Killing executors is not supported by current scheduler.\n"}]}, {"metadata": {"trusted": true}, "id": "0ec20a33", "cell_type": "code", "source": "df_pca = pca.transform(df_scaled).select('path', 'label', 'pcaFeatures')", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "id": "32017c48", "cell_type": "code", "source": "print('Explained Variance Ratio', pca.explainedVariance.toArray())\ndf_pca.show(6)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "d3e3c907", "cell_type": "markdown", "source": "<u>Rappel du PATH o\u00f9 seront inscrits les fichiers au format \"**parquet**\" <br />\ncontenant nos r\u00e9sultats, \u00e0 savoir, un DataFrame contenant 3 colonnes</u> :\n 1. Path des images\n 2. Label de l'image\n 3. Vecteur de caract\u00e9ristiques de l'image"}, {"metadata": {"trusted": true}, "id": "ecfa3bbc", "cell_type": "code", "source": "print(PATH_Result)", "execution_count": 30, "outputs": [{"name": "stdout", "output_type": "stream", "text": "/media/sf_shared_folder/data/results_local\n"}]}, {"metadata": {}, "id": "3f3c62ce", "cell_type": "markdown", "source": "<u>Enregistrement des donn\u00e9es trait\u00e9es au format \"**parquet**\"</u> :"}, {"metadata": {"scrolled": true, "trusted": true}, "id": "944af23a", "cell_type": "code", "source": "df_pca.write.mode(\"overwrite\").parquet(PATH_Result)", "execution_count": 31, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2023-02-26 12:25:26.642229: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-02-26 12:25:26.799458: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-02-26 12:25:26.799916: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-02-26 12:25:26.829456: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-02-26 12:25:27.598039: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-02-26 12:25:27.598579: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-02-26 12:25:27.598679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n2023-02-26 12:25:28.737816: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2023-02-26 12:25:28.738330: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2023-02-26 12:25:28.738459: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Ubuntu22): /proc/driver/nvidia/version does not exist\n2023-02-26 12:25:28.738734: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-02-26 12:25:31.291961: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 67436544 exceeds 10% of free system memory.\n2023-02-26 12:25:31.328545: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 68646144 exceeds 10% of free system memory.\n1/1 [==============================] - 1s 1s/step\n"}, {"name": "stdout", "output_type": "stream", "text": "23/02/26 12:25:31 ERROR Utils: Aborting task\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_59239/61540108.py\", line 43, in featurize_udf\n  File \"/tmp/ipykernel_59239/61540108.py\", line 21, in featurize_series\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/__init__.py\", line 135, in wrapper\n    return func(self, **kwargs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/feature.py\", line 6202, in __init__\n    self._java_obj = self._new_java_obj(\"org.apache.spark.ml.feature.PCA\", self.uid)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 80, in _new_java_obj\n    assert sc is not None\nAssertionError\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n23/02/26 12:25:31 ERROR FileFormatWriter: Job job_202302261225263203359195525304166_0026 aborted.\n23/02/26 12:25:31 ERROR Executor: Exception in task 0.0 in stage 26.0 (TID 126)\norg.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_59239/61540108.py\", line 43, in featurize_udf\n  File \"/tmp/ipykernel_59239/61540108.py\", line 21, in featurize_series\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/__init__.py\", line 135, in wrapper\n    return func(self, **kwargs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/feature.py\", line 6202, in __init__\n    self._java_obj = self._new_java_obj(\"org.apache.spark.ml.feature.PCA\", self.uid)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 80, in _new_java_obj\n    assert sc is not None\nAssertionError\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\t... 9 more\n23/02/26 12:25:31 WARN TaskSetManager: Lost task 0.0 in stage 26.0 (TID 126) (192.168.1.124 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_59239/61540108.py\", line 43, in featurize_udf\n  File \"/tmp/ipykernel_59239/61540108.py\", line 21, in featurize_series\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/__init__.py\", line 135, in wrapper\n    return func(self, **kwargs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/feature.py\", line 6202, in __init__\n    self._java_obj = self._new_java_obj(\"org.apache.spark.ml.feature.PCA\", self.uid)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 80, in _new_java_obj\n    assert sc is not None\nAssertionError\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\t... 9 more\n\n23/02/26 12:25:31 ERROR TaskSetManager: Task 0 in stage 26.0 failed 1 times; aborting job\n23/02/26 12:25:31 WARN TaskSetManager: Lost task 1.0 in stage 26.0 (TID 127) (192.168.1.124 executor driver): TaskKilled (Stage cancelled)\n23/02/26 12:25:31 ERROR FileFormatWriter: Aborting job f826538c-b665-49b0-ab3e-0284b9cfd1ed.\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 126) (192.168.1.124 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_59239/61540108.py\", line 43, in featurize_udf\n  File \"/tmp/ipykernel_59239/61540108.py\", line 21, in featurize_series\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/__init__.py\", line 135, in wrapper\n    return func(self, **kwargs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/feature.py\", line 6202, in __init__\n    self._java_obj = self._new_java_obj(\"org.apache.spark.ml.feature.PCA\", self.uid)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 80, in _new_java_obj\n    assert sc is not None\nAssertionError\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\t... 9 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_59239/61540108.py\", line 43, in featurize_udf\n  File \"/tmp/ipykernel_59239/61540108.py\", line 21, in featurize_series\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/__init__.py\", line 135, in wrapper\n    return func(self, **kwargs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/feature.py\", line 6202, in __init__\n    self._java_obj = self._new_java_obj(\"org.apache.spark.ml.feature.PCA\", self.uid)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 80, in _new_java_obj\n    assert sc is not None\nAssertionError\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\t... 9 more\n"}, {"ename": "Py4JJavaError", "evalue": "An error occurred while calling o116.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 126) (192.168.1.124 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_59239/61540108.py\", line 43, in featurize_udf\n  File \"/tmp/ipykernel_59239/61540108.py\", line 21, in featurize_series\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/__init__.py\", line 135, in wrapper\n    return func(self, **kwargs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/feature.py\", line 6202, in __init__\n    self._java_obj = self._new_java_obj(\"org.apache.spark.ml.feature.PCA\", self.uid)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 80, in _new_java_obj\n    assert sc is not None\nAssertionError\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\t... 9 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\t... 42 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_59239/61540108.py\", line 43, in featurize_udf\n  File \"/tmp/ipykernel_59239/61540108.py\", line 21, in featurize_series\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/__init__.py\", line 135, in wrapper\n    return func(self, **kwargs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/feature.py\", line 6202, in __init__\n    self._java_obj = self._new_java_obj(\"org.apache.spark.ml.feature.PCA\", self.uid)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 80, in _new_java_obj\n    assert sc is not None\nAssertionError\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\t... 9 more\n", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "Cell \u001b[0;32mIn [31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfeatures_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_Result\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n", "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n", "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n", "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o116.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 126) (192.168.1.124 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_59239/61540108.py\", line 43, in featurize_udf\n  File \"/tmp/ipykernel_59239/61540108.py\", line 21, in featurize_series\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/__init__.py\", line 135, in wrapper\n    return func(self, **kwargs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/feature.py\", line 6202, in __init__\n    self._java_obj = self._new_java_obj(\"org.apache.spark.ml.feature.PCA\", self.uid)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 80, in _new_java_obj\n    assert sc is not None\nAssertionError\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\t... 9 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\t... 42 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_59239/61540108.py\", line 43, in featurize_udf\n  File \"/tmp/ipykernel_59239/61540108.py\", line 21, in featurize_series\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/__init__.py\", line 135, in wrapper\n    return func(self, **kwargs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/feature.py\", line 6202, in __init__\n    self._java_obj = self._new_java_obj(\"org.apache.spark.ml.feature.PCA\", self.uid)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 80, in _new_java_obj\n    assert sc is not None\nAssertionError\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\t... 9 more\n"]}]}, {"metadata": {}, "id": "ed3dc074", "cell_type": "markdown", "source": "On met tout cela sous forme de fonctions pour favoriser la lisibilit\u00e9 et automatiser le traitement qui sera lancer dans le Cloud AWS (sur notre cluster EMR). Les donn\u00e9es seront alors export\u00e9es et stock\u00e9es sur S3. "}, {"metadata": {"trusted": true}, "id": "9a7bd3bf", "cell_type": "code", "source": "def update_schema(df):\n    \n    # Define a UDF to convert the features column to a list\n    to_list_udf = udf(lambda row: [float(i) for i in row], ArrayType(FloatType()))\n\n    # Convert the features column to a list and create a new column \"features_list\"\n    features_df = features_df.withColumn(\"features_list\", to_list_udf(features_df[\"features\"]))\n\n    # Define UDF to convert list to DenseVector\n    to_dense_vector_udf = udf(lambda x: Vectors.dense(x), VectorUDT())\n\n    # Convert the \"features_list\" column to DenseVector and create a new column \"features_vec\"\n    features_df = features_df.withColumn(\"features_vec\", to_dense_vector_udf(features_df[\"features_list\"]))\n\n    # Drop the \"features_list\" column\n    features_df = features_df.drop(\"features_list\")\n    \n    return features_df\n\ndef scaler(df):\n    \n    scaler = StandardScaler(\n        inputCol = 'features_vec', \n        outputCol = 'scaledFeatures',\n        withMean = True,\n        withStd = True\n    ).fit(features_df)\n\n    # when we transform the dataframe, the old\n    # feature will still remain in it\n    features_df = scaler.transform(features_df)\n\n    return features_df\n\ndef PCA(df):\n    \n    n_components = 2\n    pca = PCA(\n        k = n_components, \n        inputCol = 'scaledFeatures', \n        outputCol = 'pcaFeatures'\n    ).fit(df_scaled)\n    \n    df_pca = pca.transform(df_scaled).select('path', 'label', 'pcaFeatures')\n    \n    return df_pca\n\ndef export_Parquet(df, export_path):\n    \n    df_pca.write.mode(\"overwrite\").parquet(PATH_Result)\n    df_pca = pd.read_parquet(PATH_Result, engine='pyarrow')\n    \n    return df_pca", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1678659371640_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-32-58.eu-west-1.compute.internal:20888/proxy/application_1678659371640_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-38-124.eu-west-1.compute.internal:8042/node/containerlogs/container_1678659371640_0001_01_000001/livy\">Link</a></td><td>None</td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {}, "id": "38b86047", "cell_type": "markdown", "source": "## 3.8 Chargement des donn\u00e9es enregistr\u00e9es et validation du r\u00e9sultat\n\n<u>On charge les donn\u00e9es fraichement enregistr\u00e9es dans un **DataFrame Pandas**</u> :"}, {"metadata": {"trusted": true}, "id": "822fc4da", "cell_type": "code", "source": "df_pca = pd.read_parquet(PATH_Result, engine='pyarrow')", "execution_count": 32, "outputs": []}, {"metadata": {}, "id": "7d0082a0", "cell_type": "markdown", "source": "<u>On affiche les 5 premi\u00e8res lignes du DataFrame</u> :"}, {"metadata": {"trusted": true}, "id": "84c5c727", "cell_type": "code", "source": "print(df_pca.shape)\ndf_pca.head()", "execution_count": 33, "outputs": [{"name": "stdout", "output_type": "stream", "text": "(0, 0)\n"}, {"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>", "text/plain": "Empty DataFrame\nColumns: []\nIndex: []"}, "execution_count": 33, "metadata": {}, "output_type": "execute_result"}]}, {"metadata": {}, "id": "fcc368a2", "cell_type": "markdown", "source": "<u>On valide que la dimension du vecteur de caract\u00e9ristiques des images est bien de dimension 1280</u> :"}, {"metadata": {"trusted": true}, "id": "9535fb2b", "cell_type": "code", "source": "df_pca.loc[0,'features'].shape", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "0fcca1e8", "cell_type": "markdown", "source": "Nous venons de valider le processus sur un jeu de donn\u00e9es all\u00e9g\u00e9 en local <br />\no\u00f9 nous avons simul\u00e9 un cluster de machines en r\u00e9partissant la charge de travail <br />\nsur diff\u00e9rents c\u0153urs de processeur au sein d'une m\u00eame machine.\n\nNous allons maintenant g\u00e9n\u00e9raliser le processus en d\u00e9ployant notre solution <br />\nsur un r\u00e9el cluster de machines et nous travaillerons d\u00e9sormais sur la totalit\u00e9 <br />\ndes 22 819 images de notre dossier \"Test\"."}, {"metadata": {}, "id": "b2f90744", "cell_type": "markdown", "source": "# 4. D\u00e9ploiement de la solution sur le cloud\n\nMaintenant que nous avons v\u00e9rifi\u00e9 que notre solution fonctionne, <br />\nil est temps de la <u>d\u00e9ployer \u00e0 plus grande \u00e9chelle sur un vrai cluster de machines</u>.\n\n**Attention**, *je travaille sous Linux avec une version Ubuntu, <br />\nles commandes d\u00e9crites ci-dessous sont donc r\u00e9alis\u00e9es <br />\nexclusivement dans cet environnement.*\n\n<u>Plusieurs contraintes se posent</u> :\n 1. Quel prestataire de Cloud choisir ?\n 2. Quelles solutions de ce prestataire adopter ?\n 3. O\u00f9 stocker nos donn\u00e9es ?\n 4. Comment configurer nos outils dans ce nouvel environnement ?\n \n## 4.1 Choix du prestataire cloud : AWS\n\nLe prestataire le plus connu et qui offre \u00e0 ce jour l'offre <br />\nla plus large dans le cloud computing est **Amazon Web Services** (AWS).<br />\nCertaines de leurs offres sont parfaitement adapt\u00e9es \u00e0 notre probl\u00e9matique <br />\net c'est la raison pour laquelle j'utiliserai leurs services.\n\nL'objectif premier est de pouvoir, gr\u00e2ce \u00e0 AWS, <u>louer de la puissance de calcul \u00e0 la demande</u>. <br />\nL'id\u00e9e \u00e9tant de pouvoir, quel que soit la charge de travail, <br />\nobtenir suffisamment de puissance de calcul pour pouvoir traiter nos images, <br />\nm\u00eame si le volume de donn\u00e9es venait \u00e0 fortement augmenter.\n\nDe plus, la capacit\u00e9 d'utiliser cette puissance de calcul \u00e0 la demande <br />\npermet de diminuer drastiquement les co\u00fbts si l'on compare les co\u00fbts d'une location <br />\nde serveur complet sur une dur\u00e9e fixe (1 mois, 1 ann\u00e9e par exemple).\n\n## 4.2 Choix de la solution technique : EMR\n\n<u>Plusieurs solutions s'offre \u00e0 nous</u> :\n1. Solution **IAAS** (Infrastructure AS A Service)\n - Dans cette configuration **AWS** met \u00e0 notre disposition des serveurs vierges <br />\n   sur lequel nous avons un acc\u00e8s en administrateur, ils sont nomm\u00e9s **instance EC2**.<br />\n   Pour faire simple, nous pouvons avec cette solution reproduire pratiquement <br />\n   \u00e0 l'identique la solution mise en \u0153uvre en local sur notre machine.<br />\n   <u>On installe nous-m\u00eame l'int\u00e9gralit\u00e9 des outils puis on ex\u00e9cute notre script</u> :\n  - Installation de **Spark**, **Java** etc.\n  - Installation de **Python** (via Anaconda par exemple)\n  - Installation de **Jupyter Notebook**\n  - Installation des **librairies compl\u00e9mentaires**\n  - Il faudra bien \u00e9videment veiller \u00e0 **impl\u00e9menter les librairies \n    n\u00e9cessaires \u00e0 toutes les machines (workers) du cluster**\n  - <u>Avantages</u> :\n      - Libert\u00e9 totale de mise en \u0153uvre de la solution\n      - Facilit\u00e9 de mise en \u0153uvre \u00e0 partir d'un mod\u00e8le qui s'ex\u00e9cute en local sur une machine Linux\n  - <u>Inconv\u00e9nients</u> :\n      - Cronophage\n          - N\u00e9cessit\u00e9 d'installer et de configurer toute la solution\n      - Possible probl\u00e8mes techniques \u00e0 l'installation des outils (des probl\u00e9matiques qui <br />\n        n'existaient pas en local sur notre machine peuvent apparaitre sur le serveur EC2)\n      - Solution non p\u00e9renne dans le temps, il faudra veiller \u00e0 la mise \u00e0 jour des outils <br />\n        et \u00e9ventuellement devoir r\u00e9installer Spark, Java etc. \n2. Solution **PAAS** (Plateforme As A Service)\n - **AWS** fournit \u00e9norm\u00e9ment de services diff\u00e9rents, dans l'un de ceux-l\u00e0 <br />\n   il existe une offre qui permet de louer des **instances EC2** <br />\n   avec des applications pr\u00e9install\u00e9es et configur\u00e9es : il s'agit du **service EMR**.\n - **Spark** y sera d\u00e9j\u00e0 install\u00e9\n - Possibilit\u00e9 de demander l'installation de **Tensorflow** ainsi que **JupyterHub**\n - Possibilit\u00e9 d'indiquer des **packages compl\u00e9mentaires** \u00e0 installer <br />\n   \u00e0 l'initialisation du serveur **sur l'ensemble des machines du cluster**.\n - <u>Avantages</u> :\n     - Facilit\u00e9 de mise en \u0153uvre\n         - Il suffit de tr\u00e8s peu de configuration pour obtenir <br />\n           un environnement parfaitement fonctionnel\n     - Rapidit\u00e9 de mise en \u0153uvre\n         - Une fois la premi\u00e8re configuration r\u00e9alis\u00e9e, il est tr\u00e8s facile <br />\n           et tr\u00e8s rapide de recr\u00e9er des clusters \u00e0 l'identique qui seront <br />\n           disponibles presque instantan\u00e9ment (le temps d'instancier les <br />\n           serveurs soit environ 15/20 minutes)\n     - Solutions mat\u00e9rielless et logicielles optimis\u00e9es par les ing\u00e9nieurs d'AWS\n         - On sait que les versions install\u00e9es vont fonctionner <br />\n           et que l'architecture propos\u00e9e est optimis\u00e9e\n     - Stabilit\u00e9 de la solution\n    - Solution \u00e9volutive\n        Il est facile d\u2019obtenir \u00e0 chaque nouvelle instanciation une version \u00e0 jour <br />\n        de chaque package, en \u00e9tant garanti de leur compatibilit\u00e9 avec le reste de l\u2019environnement.\n  - Plus s\u00e9curis\u00e9\n\t- Les \u00e9ventuels patchs de s\u00e9curit\u00e9 seront automatiquement mis \u00e0 jour <br />\n      \u00e0 chaque nouvelle instanciation du cluster EMR.\n - <u>Inconv\u00e9nients</u> :\n     - Peut-\u00eatre un certain manque de libert\u00e9 sur la version des packages disponibles ? <br />\n       M\u00eame si je n'ai pas constat\u00e9 ce probl\u00e8me.\n   \n\nJe retiens la solution **PAAS** en choisissant d'utiliser <br />\nle service **EMR** d'Amazon Web Services.<br />\nJe la trouve plus adapt\u00e9e \u00e0 notre probl\u00e9matique et permet <br />\nune mise en \u0153uvre qui soit \u00e0 la fois plus rapide et <br />\nplus efficace que la solution IAAS.\n\n## 4.3 Choix de la solution de stockage des donn\u00e9es : Amazon S3\n\n<u>Amazon propose une solution tr\u00e8s efficace pour la gestion du stockage des donn\u00e9es</u> : **Amazon S3**. <br />\nS3 pour Amazon Simple Storage Service.\n\nIl pourrait \u00eatre tentant de stocker nos donn\u00e9es sur l'espace allou\u00e9 par le serveur **EC2**, <br />\nmais si nous ne prenons aucune mesure pour les sauvegarder ensuite sur un autre support, <br />\n<u>les donn\u00e9es seront perdues</u> lorsque le serveur sera r\u00e9sili\u00e9 (on r\u00e9silie le serveur lorsqu'on <br />\nne s'en sert pas pour des raisons de co\u00fbt).<br />\nDe fait, si l'on d\u00e9cide d'utiliser l'espace disque du serveur EC2 il faudra imaginer <br />\nune solution pour sauvegarder les donn\u00e9es avant la r\u00e9siliation du serveur.\nDe plus, nous serions expos\u00e9s \u00e0 certaines probl\u00e9matiques si nos donn\u00e9es venaient \u00e0 <br />\n**saturer** l'espace disponible de nos serveurs (ralentissements, disfonctionnements).\n\n<u>Utiliser **Amazon S3** permet de s'affranchir de toutes ces probl\u00e9matiques</u>. <br />\nL'espace disque disponible est **illimit\u00e9**, et il est **ind\u00e9pendant de nos serveurs EC2**. <br />\nL'acc\u00e8s aux donn\u00e9es est **tr\u00e8s rapide** car nous restons dans l'environnement d'AWS <br />\net nous prenons soin de <u>choisir la m\u00eame r\u00e9gion pour nos serveurs **EC2** et **S3**</u>.\n\nDe plus, comme nous le verrons <u>il est possible d'acc\u00e9der aux donn\u00e9es sur **S3** <br />\n    de la m\u00eame mani\u00e8re que l'on **acc\u00e8de aux donn\u00e9es sur un disque local**</u>.<br />\nNous utiliserons simplement un **PATH au format s3://...** .\n\n## 4.4 Configuration de l'environnement de travail\n\nLa premi\u00e8re \u00e9tape est d'installer et de configurer [**AWS Cli**](https://aws.amazon.com/fr/cli/),<br />\nil s'agit de l'**interface en ligne de commande d'AWS**.<br />\nElle nous permet d'**interagir avec les diff\u00e9rents services d'AWS**, comme **S3** par exemple.\n\nPour pouvoir utiliser **AWS Cli**, il faut le configurer en cr\u00e9ant pr\u00e9alablement <br />\nun utilisateur \u00e0 qui on donnera les autorisations dont nous aurons besoin.<br />\nDans ce projet il faut que l'utilisateur ait \u00e0 minima un contr\u00f4le total sur le service S3.\n\n<u>La gestion des utilisateurs et de leurs droits s'effectue via le service **IAM**</u> d'AWS.\n\nUne fois l'utilisateur cr\u00e9\u00e9 et ses autorisations configur\u00e9es nous cr\u00e9ons une **paire de cl\u00e9s** <br />\nqui nous permettra de nous **connecter sans \u00e0 avoir \u00e0 devoir saisir syst\u00e9matiquement notre login/mot de passe**.<br />\n\nIl faut \u00e9galement configurer l'**acc\u00e8s SSH** \u00e0 nos futurs serveurs EC2. <br />\nIci aussi, via un syst\u00e8me de cl\u00e9s publique / priv\u00e9e qui nous dispense de devoir nous authentifier \"\u00e0 la main\" \u00e0 chaque connexion.\n\nToutes ses \u00e9tapes de configuration sont parfaitement d\u00e9crites <br />\ndans le cours du projet: [R\u00e9alisez des calculs distribu\u00e9s sur des donn\u00e9es massives / D\u00e9couvrez Amazon Web Services](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308686-decouvrez-amazon-web-services#/id/r-4355822)\n\n## 4.5 Upload de nos donn\u00e9es sur S3\n\nNos outils sont configur\u00e9s. <br />\nIl faut maintenant uploader nos donn\u00e9es de travail sur Amazon S3.\n\nIci aussi les \u00e9tapes sont d\u00e9crites avec pr\u00e9cision <br />\ndans le cours [R\u00e9alisez des calculs distribu\u00e9s sur des donn\u00e9es massives / Stockez des donn\u00e9es sur S3](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308691-stockez-des-donnees-sur-s3)\n\nJe d\u00e9cide de n'uploader que les donn\u00e9es contenues dans le dossier **Test** du [jeu de donn\u00e9es du projet](https://www.kaggle.com/moltean/fruits/download)\n\n\nLa premi\u00e8re \u00e9tape consiste \u00e0 **cr\u00e9er un bucket sur S3** <br />\ndans lequel nous uploaderons les donn\u00e9es du projet:\n- **aws s3 mb s3://p8-bucket**\n\nOn v\u00e9rifie que le bucket \u00e0 bien \u00e9t\u00e9 cr\u00e9\u00e9\n- **aws s3 ls**\n - Si le nom du bucket s'affiche alors c'est qu'il a \u00e9t\u00e9 correctement cr\u00e9\u00e9.\n\nOn copie ensuite le contenu du dossier \"**test**\" <br />\ndans un r\u00e9pertoire \"**test**\" sur notre bucket \"**p8-bucket**\":\n1. On se place \u00e0 l'int\u00e9rieur du r\u00e9pertoire **Test**\n2. **aws s3 sync . s3://p8-bucket/Test**\n\nLa commande **sync** est utile pour synchroniser deux r\u00e9pertoires.\n\n<u>Nos donn\u00e9es du projet sont maintenant disponibles sur Amazon S3</u>.\n\n## 4.6 Configuration du serveur EMR\n\nUne fois encore, le cours [R\u00e9alisez des calculs distribu\u00e9s sur des donn\u00e9es massives / D\u00e9ployez un cluster de calculs distribu\u00e9s](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues) <br /> d\u00e9taille l'essentiel des \u00e9tapes pour lancer un cluster avec **EMR**.\n\n<u>Je d\u00e9taillerai ici les \u00e9tapes particuli\u00e8res qui nous permettent <br />\nde configurer le serveur selon nos besoins</u> :\n\n1. Cliquez sur Cr\u00e9er un cluster\n2. Cliquez sur Acc\u00e9der aux options avanc\u00e9es\n\n### 4.6.1 \u00c9tape 1 : Logiciels et \u00e9tapes\n\n#### 4.6.1.1 Configuration des logiciels\n\n<u>S\u00e9lectionnez les packages dont nous aurons besoin comme dans la capture d'\u00e9cran</u> :\n1. Nous s\u00e9lectionnons la version **6.7.0** d'**EMR** afin d'\u00e9viter tout soucis de comptabilit\u00e9 des packages\n2. Nous cochons bien \u00e9videment **Hadoop** et **Spark** qui seront pr\u00e9install\u00e9s dans leur version la plus r\u00e9cente\n3. Nous aurons \u00e9galement besoin de **TensorFlow** pour importer notre mod\u00e8le et r\u00e9aliser le **transfert learning**\n4. Nous travaillerons enfin avec un **notebook Jupyter** via l'application **JupyterHub**<br />\n - Comme nous le verrons dans un instant nous allons <u>param\u00e9trer l'application afin que les notebooks</u>, <br />\n   comme le reste de nos donn\u00e9es de travail, <u>soient enregistr\u00e9s directement sur S3</u>.\n\n#### 4.6.1.2 Modifier les param\u00e8tres du logiciel\n\n<u>Param\u00e9trez la persistance des notebooks cr\u00e9\u00e9s et ouvert via JupyterHub</u> :\n- On peut \u00e0 cette \u00e9tape effectuer des demandes de param\u00e9trage particuli\u00e8res sur nos applications. <br />\n  L'objectif est, comme pour le reste de nos donn\u00e9es de travail, <br />\n  d'\u00e9viter toutes les probl\u00e9matiques \u00e9voqu\u00e9es pr\u00e9c\u00e9demment. <br />\n  C'est l'objectif \u00e0 cette \u00e9tape, <u>nous allons enregistrer <br />\n  et ouvrir les notebooks</u> non pas sur l'espace disque de  l'instance EC2 (comme <br />\n  ce serait le cas dans la configuration par d\u00e9faut de JupyterHub) mais <br />\n  <u>directement sur **Amazon S3**</u>.\n- <u>deux solutions sont possibles pour r\u00e9aliser cela</u> :\n 1. Cr\u00e9er un **fichier de configuration JSON** que l'on **upload sur S3** et on indique ensuite le chemin d\u2019acc\u00e8s au fichier JSON\n 2. Rentrez directement la configuration au format JSON\n \nJ'ai personnellement cr\u00e9\u00e9 un fichier JSON lors de la cr\u00e9ation de ma premi\u00e8re instance EMR, <br />\npuis lorsqu'on d\u00e9cide de cloner notre serveur pour en recr\u00e9er un facilement \u00e0 l'identique, <br />\nla configuration du fichier JSON se retrouve directement copi\u00e9 comme ci-dessous.\n\n<u>Voici le contenu de mon fichier JSON</u> : \n\n[\n  {\n    \"Classification\": \"jupyter-s3-conf\",\n    \"Properties\": {\n      \"s3.persistence.bucket\": \"p8-bucket\",\n      \"s3.persistence.enabled\": \"true\"\n    }\n  }\n]\n\n Appuyez ensuite sur \"**Suivant**\"\n\n### 4.6.2 \u00c9tape 2 : Mat\u00e9riel\n\nA cette \u00e9tape, laissez les choix par d\u00e9faut. <br />\n<u>L'important ici est la s\u00e9lection de nos instances</u> :\n\n1. je choisis les instances de type **M5** qui sont des **instances de type \u00e9quilibr\u00e9s**\n2. je choisis le type **xlarge** qui est l'instance la **moins on\u00e9reuse disponible**\n [Plus d'informations sur les instances M5 Amazon EC2](https://aws.amazon.com/fr/ec2/instance-types/m5/)\n3. Je s\u00e9lectionne **1 instance Ma\u00eetre** (le driver) et **2 instances Principales** (les workeurs) <br />\n   soit **un total de 3 instances EC2**.\n\n### 4.6.3 \u00c9tape 3 : Param\u00e8tres de cluster g\u00e9n\u00e9raux\n\n#### 4.6.3.1 Options g\u00e9n\u00e9rales\n<u>La premi\u00e8re chose \u00e0 faire est de donner un nom au cluster</u> :<br />\nJ'ai conserv\u00e9 le param\u00e9trage par d\u00e9faut concernant la r\u00e9siliation automatique du cluster (apr\u00e8s 1h d'inactivit\u00e9).\n\n#### 4.6.3.2 Actions d'amor\u00e7age\n\nNous allons \u00e0 cette \u00e9tape **choisir les packages manquants \u00e0 installer** et qui <br />\nnous serons utiles dans l'ex\u00e9cution de notre notebook.<br />\n<u>L'avantage de r\u00e9aliser cette \u00e9tape maintenant est que les packages <br />\ninstall\u00e9s le seront sur l'ensemble des machines du cluster</u>.\n\nLa proc\u00e9dure pour cr\u00e9er le fichier **bootstrap** qui contient <br />\nl'ensemble des instructions permettant d'installer tous <br />\nles packages dont nous aurons besoin est expliqu\u00e9 dans <br />\nle cours [R\u00e9alisez des calculs distribu\u00e9s sur des donn\u00e9es massives / Bootstrapping](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues#/id/r-4356490)\n\nNous cr\u00e9ons donc un fichier nomm\u00e9 \"**bootstrap-emr.sh**\" que nous <u>uploadons <br />\nsur S3</u>(je l\u2019installe \u00e0 la racine de mon **bucket \"p8-bucket\"**) et nous l'ajoutons<br />\naux actions d'amor\u00e7age via l'interface. <br />\n\nLe fichier **bootstrap-emr.sh** contient simplement des commandes \"**pip install**\"<br />\npour **installer les biblioth\u00e8ques manquantes** comme r\u00e9alis\u00e9 en local.<br />\n\nUne fois encore, <u>il est n\u00e9cessaire de r\u00e9aliser ces actions \u00e0 cette \u00e9tape</u> <br />\npour que <u>les packages soient install\u00e9s sur l'ensemble des machines du cluster</u> <br />\net non pas uniquement sur le driver, comme cela serait le cas si nous ex\u00e9cutions <br />\nces commandes directement dans le notebook JupyterHub ou dans la console EMR (connect\u00e9 au driver).\n\n**setuptools** et **pip** sont mis \u00e0 jour pour \u00e9viter une probl\u00e9matique <br />\navec l'installation du package **pyarrow**.<br />\n\n\n### 4.6.4 \u00c9tape 4 : S\u00e9curit\u00e9\n\n#### 4.6.4.1 Options de s\u00e9curit\u00e9\n\nA cette \u00e9tape nous s\u00e9lectionnons la **paire de cl\u00e9s EC2** cr\u00e9\u00e9 pr\u00e9c\u00e9demment. <br />\nElle nous permettra de se connecter en **ssh** \u00e0 nos **instances EC2** <br />\nsans avoir \u00e0 entrer nos login/mot de passe.<br />\nOn laisse les autres param\u00e8tres par d\u00e9faut. <br />\nEt enfin, on clique sur \"***Cr\u00e9er un cluster***\"\n \n\n## 4.7 Instanciation du serveur\n\nIl ne nous reste plus qu'\u00e0 attendre que le serveur soit pr\u00eat. <br />\nCette \u00e9tape peut prendre entre **15 et 20 minutes**.\n\n<u>Plusieurs \u00e9tapes s'encha\u00eene, on peut suivre l'avanc\u00e9 du statut du **cluster EMR**</u>\n\n<u>Lorsque le statut affiche en vert: \"**En attente**\" cela signifie que l'instanciation <br />\ns'est bien d\u00e9roul\u00e9e et que notre serveur est pr\u00eat \u00e0 \u00eatre utilis\u00e9</u>. \n\n## 4.8 Cr\u00e9ation du tunnel SSH \u00e0 l'instance EC2 (Ma\u00eetre)\n\n### 4.8.1 Cr\u00e9ation des autorisations sur les connexions entrantes\n\n<u>Nous souhaitons maintenant pouvoir acc\u00e9der \u00e0 nos applications</u> :\n - **JupyterHub** pour l'ex\u00e9cution de notre notebook\n - **Serveur d'historique Spark** pour le suivi de l'ex\u00e9cution <br />\n   des t\u00e2ches de notre script lorsqu'il sera lanc\u00e9\n \nCependant, <u>ces applications ne sont accessibles que depuis le r\u00e9seau local du driver</u>, <br />\net pour y acc\u00e9der nous devons **cr\u00e9er un tunnel SSH vers le driver**.\n\nPar d\u00e9faut, ce driver se situe derri\u00e8re un firewall qui bloque l'acc\u00e8s en SSH. <br />\n<u>Pour ouvrir le port 22 qui correspond au port sur lequel \u00e9coute le serveur SSH, <br />\nil faut modifier le **groupe de s\u00e9curit\u00e9 EC2 du driver**</u>.\n\nCette \u00e9tape est d\u00e9crite dans le cours [R\u00e9alisez des calculs distribu\u00e9s sur des donn\u00e9es massives / Lancement d'une application \u00e0 partir du driver](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues#/id/r-4356512): \n\n*Il faudra que l'on se connecte en SSH au driver de notre cluster. <br />\nPar d\u00e9faut, ce driver se situe derri\u00e8re un firewall qui bloque l'acc\u00e8s en SSH. <br />\nPour ouvrir le port 22 qui correspond au port sur lequel \u00e9coute le serveur SSH, <br />\nil faut modifier le groupe de s\u00e9curit\u00e9 EC2 du driver. Sur la page de la console <br />\nconsacr\u00e9e \u00e0 EC2, dans l'onglet \"R\u00e9seau et s\u00e9curit\u00e9\", cliquez sur \"Groupes de s\u00e9curit\u00e9\". <br />\nVous allez devoir modifier le groupe de s\u00e9curit\u00e9 d\u2019ElasticMapReduce-Master. <br />\nDans l'onglet \"Entrant\", ajoutez une r\u00e8gle SSH dont la source est \"N'importe o\u00f9\" <br />\n(ou \"Mon IP\" si vous disposez d'une adresse IP fixe).*\n\n![Configuration autorisation ports entrants pour ssh](img/EMR_config_ssh_01.png)\n\n<u>Une fois cette \u00e9tape r\u00e9alis\u00e9e vous devriez avoir une configuration semblable \u00e0 la mienne</u> :\n\n![Configuration ssh termin\u00e9e](img/EMR_config_ssh_02.png)\n\n### 4.8.2 Cr\u00e9ation du tunnel ssh vers le Driver\n\nOn peut maintenant \u00e9tablir le **tunnel SSH** vers le **Driver**. <br />\nPour cela on r\u00e9cup\u00e8re les informations de connexion fournis par Amazon <br />\ndepuis la page du service EMR / Cluster / onglet R\u00e9capitulatif en <br />\ncliquant sur \"**Activer la connexion Web**\"\n\n![Activer la connexion Web](img/EMR_tunnel_ssh_01.png)\n\n<u>On r\u00e9cup\u00e8re ensuite la commande fournis par Amazon pour **\u00e9tablir le tunnel SSH**</u> :\n\n![R\u00e9cup\u00e9rer la commande pour \u00e9tablir le tunnel ssh](img/EMR_tunnel_ssh_02.png)\n\n<u>Dans mon cas, la commande ne fonctionne pas tel</u> quel et j'ai du **l'adapter \u00e0 ma configuration**. <br />\nLa **cl\u00e9 ssh** se situe dans un dossier \"**.ssh**\" elle-m\u00eame situ\u00e9e dans <br />\nmon **r\u00e9pertoire personnel** dont le symbole est, sous Linux, identifi\u00e9 par un tilde \"**~**\".\n\nAyant suivi le cours [R\u00e9alisez des calculs distribu\u00e9s sur des donn\u00e9es massives / Lancement d'une application \u00e0 partir du driver](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives) <br />\nj'ai choisi d'utiliser le port **5555** au lieu du **8157**, m\u00eame si le choix n'est pas tr\u00e8s important.<br />\n    j'ai \u00e9galement rencontr\u00e9 un <u>probl\u00e8me de compatibilit\u00e9</u> avec <br />\nl'argument \"**-N**\" (liste des arguments et leur significations <br />\ndisponibles [ici](https://explainshell.com/explain?cmd=ssh+-L+-N+-f+-l+-D)) j'ai d\u00e9cid\u00e9 de simplement le supprimer.\n\n<u>Finalement, j'utilise la commande suivante dans un terminal pour \u00e9tablir <br />\n    mon tunnel ssh (seul l'URL change d'une instance \u00e0 une autre)</u> : <br />\n\"**ssh -i \"~/VirtualBox VMs/Ubuntu/shared_folder/p8-key-pair-ec2.pem\" -D 5555 hadoop@ec2-3-253-97-196.eu-west-1.compute.amazonaws.com**\"\n\n<u>On inscrit \"**yes**\" pour valider la connexion et si <br />\n    la connexion est \u00e9tablit on obtient le r\u00e9sultat suivant</u> :\n\n![Cr\u00e9ation du tunnel SSH](img/EMR_connexion_ssh_01.png)\n\nNous avons **correctement \u00e9tabli le tunnel ssh avec le driver** sur le port \"5555\".\n\n### 4.8.3 Configuration de FoxyProxy\n\nUne derni\u00e8re \u00e9tape est n\u00e9cessaire pour acc\u00e9der \u00e0 nos applications, <br />\nen demandant \u00e0 notre navigateur d'emprunter le tunnel ssh.<br />\nJ'utilise pour cela **FoxyProxy**.\n[Une fois encore, vous pouvez utiliser le cours pour le configurer](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308701-realisez-la-maintenance-dun-cluster#/id/r-4356554).\n\nSinon, ouvrez la configuration de **FoxyProxy** et <u>cliquez sur **Ajouter**</u> en haut \u00e0 gauche <br />\npuis renseigner les \u00e9l\u00e9ments comme dans la capture ci-dessous :\n\n![Configuration FoxyProxy Etape 1](img/EMR_foxyproxy_config_01.png)\n\n<u>On obtient le r\u00e9sultat ci-dessous</u> :\n\n![Configuration FoxyProxy Etape 2](img/EMR_foxyproxy_config_02.png)\n\n\n### 4.8.4 Acc\u00e8s aux applications du serveur EMR via le tunnel ssh\n\n\n<u>Avant d'\u00e9tablir notre **tunnel ssh** nous avions \u00e7a</u> :\n\n![avant tunnel ssh](img/EMR_tunnel_ssh_avant.png)\n\n<u>On active le **tunnel ssh** comme vu pr\u00e9c\u00e9demment puis on demande <br />\n\u00e0 notre navigateur de l'utiliser avec **FoxyProxy**</u> :\n\n![FoxyProxy activation](img/EMR_foxyproxy_activation.png)\n\n<u>On peut maintenant s'apercevoir que plusieurs applications nous sont accessibles</u> :\n\n![avant tunnel ssh](img/EMR_tunnel_ssh_apres.png)\n\n## 4.9 Connexion au notebook JupyterHub\n\nPour se connecter \u00e0 **JupyterHub** en vue d'ex\u00e9cuter notre **notebook**, <br />\nil faut commencer par <u>cliquer sur l'application **JupyterHub**</u> apparu <br />\ndepuis que nous avons configur\u00e9 le **tunnel ssh** et **foxyproxy** sur <br />\nnotre navigateur (actualisez la page si ce n\u2019est pas le cas).\n\n![D\u00e9marrage de JupyterHub](img/EMR_jupyterhub_connexion_01.png)\n\nOn passe les \u00e9ventuels avertissements de s\u00e9curit\u00e9 puis <br />\nnous arrivons sur une page de connexion.\n    \n<u>On se connecte avec les informations par d\u00e9faut</u> :\n - <u>login</u>: **jovyan**\n - <u>password</u>: **jupyter**\n \n![Connexion \u00e0 JupyterHub](img/EMR_jupyterhub_connexion_02.png)\n\nNous arrivons ensuite dans un dossier vierge de notebook.<br />\nIl suffit d'en cr\u00e9er un en cliquant sur \"**New**\" en haut \u00e0 droite.\n\n![Liste et cr\u00e9ation des notebook](img/EMR_jupyterhub_creer_notebooks.png)\n\nIl est \u00e9galement possible d'en <u>uploader un directement dans notre **bucket S3**</u>.\n\nGrace \u00e0 la <u>**persistance** param\u00e9tr\u00e9e \u00e0 l'instanciation du cluster <br />\nnous sommes actuellement dans l'arborescence de notre **bucket S3**</u>\n\n![Notebook stock\u00e9s sur S3](img/EMR_jupyterhub_S3.png)\n\nJe d\u00e9cide d'**importer un notebook d\u00e9j\u00e0 r\u00e9dig\u00e9 en local directement <br />\nsur S3** via la commande : *aws s3 cp P8_01_notebook_Linux_EMR.ipynb s3://oc-p8-bucket/jupyter/jovyan/P8_01_notebook_Linux_EMR.ipynb*<br />\net je l'ouvre depuis **l'interface JupyterHub**.\n\n## 4.10 Ex\u00e9cution du code\n\nJe d\u00e9cide d'ex\u00e9cuter cette partie du code depuis **JupyterHub h\u00e9berg\u00e9 sur notre cluster EMR**.<br />\nPour ne pas alourdir inutilement les explications du **notebook**, je ne r\u00e9expliquerai pas les \u00e9tapes communes <br />\nque nous avons d\u00e9j\u00e0 vues dans la premi\u00e8re partie o\u00f9 l'on a ex\u00e9cut\u00e9 le code localement sur notre machine virtuelle Ubuntu.\n\n<u>Avant de commencer</u>, il faut s'assurer d'utiliser le **kernel pyspark**.\n\n**En utilisant ce kernel, une session spark est cr\u00e9\u00e9 \u00e0 l'ex\u00e9cution de la premi\u00e8re cellule**. <br />\nIl n'est donc **plus n\u00e9cessaire d'ex\u00e9cuter le code \"spark = (SparkSession ...\"** comme lors <br />\nde l'ex\u00e9cution de notre notebook en local sur notre VM Ubuntu."}, {"metadata": {}, "id": "81fa5d88", "cell_type": "markdown", "source": "### 4.10.1 D\u00e9marrage de la session Spark"}, {"metadata": {"trusted": true}, "id": "1e7d56ff", "cell_type": "code", "source": "# L'ex\u00e9cution de cette cellule d\u00e9marre l'application Spark", "execution_count": null, "outputs": [{"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}]}, {"metadata": {}, "id": "f63e6b35", "cell_type": "markdown", "source": "<u>Affichage des informations sur la session en cours et liens vers Spark UI</u> :"}, {"metadata": {"scrolled": true, "trusted": true}, "id": "4fa0d3e3", "cell_type": "code", "source": "%%info", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "id": "9c653039", "cell_type": "code", "source": "%%configure -f\n{ \"conf\":{\n          \"spark.pyspark.python\": \"python\",\n          \"spark.pyspark.virtualenv.enabled\": \"true\",\n          \"spark.pyspark.virtualenv.type\":\"native\",\n          \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n         }\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "a78564e6", "cell_type": "markdown", "source": "### 4.10.2 Installation des packages\n\nLes packages n\u00e9cessaires ont \u00e9t\u00e9 install\u00e9 via l'\u00e9tape de **bootstrap** \u00e0 l'instanciation du serveur.\n\n### 4.10.3 Import des librairies"}, {"metadata": {"trusted": true}, "id": "b48dab6a", "cell_type": "code", "source": "import pandas as pd\nimport numpy as np\nimport io\nimport os\nimport tensorflow as tf\nfrom PIL import Image\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras import Model\nfrom pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split", "execution_count": 4, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {}, "id": "45df62ee", "cell_type": "markdown", "source": "### 4.10.4 D\u00e9finition des PATH pour charger les images et enregistrer les r\u00e9sultats\n\nNous acc\u00e9dons directement \u00e0 nos **donn\u00e9es sur S3** comme si elles \u00e9taient **stock\u00e9es localement**."}, {"metadata": {"trusted": true}, "id": "4cfbfcd9", "cell_type": "code", "source": "PATH = 's3://oc-p8-bucket'\nPATH_Data = PATH+'/Test'\nPATH_Result = PATH+'/Results'\nprint('PATH:        '+\\\n      PATH+'\\nPATH_Data:   '+\\\n      PATH_Data+'\\nPATH_Result: '+PATH_Result)", "execution_count": 8, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "PATH:        s3://oc-p8-bucket\nPATH_Data:   s3://oc-p8-bucket/Test\nPATH_Result: s3://oc-p8-bucket/Results", "name": "stdout"}]}, {"metadata": {}, "id": "aa520905", "cell_type": "markdown", "source": "### 4.10.5 Traitement des donn\u00e9es"}, {"metadata": {}, "id": "173c8d97", "cell_type": "markdown", "source": "#### 4.10.5.1 Chargement des donn\u00e9es"}, {"metadata": {"trusted": true}, "id": "5c5741aa", "cell_type": "code", "source": "images = spark.read.format(\"binaryFile\") \\\n  .option(\"pathGlobFilter\", \"*.jpg\") \\\n  .option(\"recursiveFileLookup\", \"true\") \\\n  .load(PATH_Data)", "execution_count": 10, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "id": "e3468069", "cell_type": "code", "source": "images.show(5)", "execution_count": 11, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "+--------------------+-------------------+------+--------------------+\n|                path|   modificationTime|length|             content|\n+--------------------+-------------------+------+--------------------+\n|s3://oc-p8-bucket...|2022-12-23 17:15:48|  7353|[FF D8 FF E0 00 1...|\n|s3://oc-p8-bucket...|2022-12-23 17:15:48|  7350|[FF D8 FF E0 00 1...|\n|s3://oc-p8-bucket...|2022-12-23 17:15:48|  7349|[FF D8 FF E0 00 1...|\n|s3://oc-p8-bucket...|2022-12-23 17:15:47|  7348|[FF D8 FF E0 00 1...|\n|s3://oc-p8-bucket...|2022-12-23 17:15:48|  7328|[FF D8 FF E0 00 1...|\n+--------------------+-------------------+------+--------------------+\nonly showing top 5 rows", "name": "stdout"}]}, {"metadata": {}, "id": "7fcd54a5", "cell_type": "markdown", "source": "<u>Je ne conserve que le **path** de l'image et j'ajoute <br />\n    une colonne contenant les **labels** de chaque image</u> :"}, {"metadata": {"trusted": true}, "id": "d95824ae", "cell_type": "code", "source": "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\nprint(images.printSchema())\nprint(images.select('path','label').show(5,False))", "execution_count": 12, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "root\n |-- path: string (nullable = true)\n |-- modificationTime: timestamp (nullable = true)\n |-- length: long (nullable = true)\n |-- content: binary (nullable = true)\n |-- label: string (nullable = true)\n\nNone\n+-----------------------------------------------+----------+\n|path                                           |label     |\n+-----------------------------------------------+----------+\n|s3://oc-p8-bucket/Test/Watermelon/r_106_100.jpg|Watermelon|\n|s3://oc-p8-bucket/Test/Watermelon/r_109_100.jpg|Watermelon|\n|s3://oc-p8-bucket/Test/Watermelon/r_108_100.jpg|Watermelon|\n|s3://oc-p8-bucket/Test/Watermelon/r_107_100.jpg|Watermelon|\n|s3://oc-p8-bucket/Test/Watermelon/r_95_100.jpg |Watermelon|\n+-----------------------------------------------+----------+\nonly showing top 5 rows\n\nNone", "name": "stdout"}]}, {"metadata": {}, "id": "0774fe57", "cell_type": "markdown", "source": "#### 4.10.5.2 Pr\u00e9paration du mod\u00e8le"}, {"metadata": {"trusted": true}, "id": "61f7722e", "cell_type": "code", "source": "model = MobileNetV2(weights='imagenet',\n                    include_top=True,\n                    input_shape=(224, 224, 3))", "execution_count": 13, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n\r    8192/14536120 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   49152/14536120 [..............................] - ETA: 37s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   81920/14536120 [..............................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  131072/14536120 [..............................] - ETA: 34s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  180224/14536120 [..............................] - ETA: 30s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  245760/14536120 [..............................] - ETA: 25s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  344064/14536120 [..............................] - ETA: 20s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  458752/14536120 [..............................] - ETA: 16s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  622592/14536120 [>.............................] - ETA: 13s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  851968/14536120 [>.............................] - ETA: 10s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1163264/14536120 [=>............................] - ETA: 8s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1622016/14536120 [==>...........................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2285568/14536120 [===>..........................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3203072/14536120 [=====>........................] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4505600/14536120 [========>.....................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6389760/14536120 [============>.................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8478720/14536120 [================>.............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10354688/14536120 [====================>.........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r11468800/14536120 [======================>.......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14499840/14536120 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14540800/14536120 [==============================] - 1s 0us/step", "name": "stdout"}]}, {"metadata": {"trusted": true}, "id": "325c4822", "cell_type": "code", "source": "new_model = Model(inputs=model.input,\n                  outputs=model.layers[-2].output)", "execution_count": 14, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "id": "06c4c21e", "cell_type": "code", "source": "brodcast_weights = sc.broadcast(new_model.get_weights())", "execution_count": 15, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "id": "26d8f44c", "cell_type": "code", "source": "new_model.summary()", "execution_count": 16, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\nConv1 (Conv2D)                  (None, 112, 112, 32) 864         input_1[0][0]                    \n__________________________________________________________________________________________________\nbn_Conv1 (BatchNormalization)   (None, 112, 112, 32) 128         Conv1[0][0]                      \n__________________________________________________________________________________________________\nConv1_relu (ReLU)               (None, 112, 112, 32) 0           bn_Conv1[0][0]                   \n__________________________________________________________________________________________________\nexpanded_conv_depthwise (Depthw (None, 112, 112, 32) 288         Conv1_relu[0][0]                 \n__________________________________________________________________________________________________\nexpanded_conv_depthwise_BN (Bat (None, 112, 112, 32) 128         expanded_conv_depthwise[0][0]    \n__________________________________________________________________________________________________\nexpanded_conv_depthwise_relu (R (None, 112, 112, 32) 0           expanded_conv_depthwise_BN[0][0] \n__________________________________________________________________________________________________\nexpanded_conv_project (Conv2D)  (None, 112, 112, 16) 512         expanded_conv_depthwise_relu[0][0\n__________________________________________________________________________________________________\nexpanded_conv_project_BN (Batch (None, 112, 112, 16) 64          expanded_conv_project[0][0]      \n__________________________________________________________________________________________________\nblock_1_expand (Conv2D)         (None, 112, 112, 96) 1536        expanded_conv_project_BN[0][0]   \n__________________________________________________________________________________________________\nblock_1_expand_BN (BatchNormali (None, 112, 112, 96) 384         block_1_expand[0][0]             \n__________________________________________________________________________________________________\nblock_1_expand_relu (ReLU)      (None, 112, 112, 96) 0           block_1_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_1_pad (ZeroPadding2D)     (None, 113, 113, 96) 0           block_1_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_1_depthwise (DepthwiseCon (None, 56, 56, 96)   864         block_1_pad[0][0]                \n__________________________________________________________________________________________________\nblock_1_depthwise_BN (BatchNorm (None, 56, 56, 96)   384         block_1_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_1_depthwise_relu (ReLU)   (None, 56, 56, 96)   0           block_1_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_1_project (Conv2D)        (None, 56, 56, 24)   2304        block_1_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_1_project_BN (BatchNormal (None, 56, 56, 24)   96          block_1_project[0][0]            \n__________________________________________________________________________________________________\nblock_2_expand (Conv2D)         (None, 56, 56, 144)  3456        block_1_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_2_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_2_expand[0][0]             \n__________________________________________________________________________________________________\nblock_2_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_2_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_2_depthwise (DepthwiseCon (None, 56, 56, 144)  1296        block_2_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_2_depthwise_BN (BatchNorm (None, 56, 56, 144)  576         block_2_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_2_depthwise_relu (ReLU)   (None, 56, 56, 144)  0           block_2_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_2_project (Conv2D)        (None, 56, 56, 24)   3456        block_2_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_2_project_BN (BatchNormal (None, 56, 56, 24)   96          block_2_project[0][0]            \n__________________________________________________________________________________________________\nblock_2_add (Add)               (None, 56, 56, 24)   0           block_1_project_BN[0][0]         \n                                                                 block_2_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_3_expand (Conv2D)         (None, 56, 56, 144)  3456        block_2_add[0][0]                \n__________________________________________________________________________________________________\nblock_3_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_3_expand[0][0]             \n__________________________________________________________________________________________________\nblock_3_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_3_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_3_pad (ZeroPadding2D)     (None, 57, 57, 144)  0           block_3_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_3_depthwise (DepthwiseCon (None, 28, 28, 144)  1296        block_3_pad[0][0]                \n__________________________________________________________________________________________________\nblock_3_depthwise_BN (BatchNorm (None, 28, 28, 144)  576         block_3_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_3_depthwise_relu (ReLU)   (None, 28, 28, 144)  0           block_3_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_3_project (Conv2D)        (None, 28, 28, 32)   4608        block_3_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_3_project_BN (BatchNormal (None, 28, 28, 32)   128         block_3_project[0][0]            \n__________________________________________________________________________________________________\nblock_4_expand (Conv2D)         (None, 28, 28, 192)  6144        block_3_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_4_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_4_expand[0][0]             \n__________________________________________________________________________________________________\nblock_4_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_4_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_4_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_4_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_4_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_4_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_4_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_4_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_4_project (Conv2D)        (None, 28, 28, 32)   6144        block_4_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_4_project_BN (BatchNormal (None, 28, 28, 32)   128         block_4_project[0][0]            \n__________________________________________________________________________________________________\nblock_4_add (Add)               (None, 28, 28, 32)   0           block_3_project_BN[0][0]         \n                                                                 block_4_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_5_expand (Conv2D)         (None, 28, 28, 192)  6144        block_4_add[0][0]                \n__________________________________________________________________________________________________\nblock_5_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_5_expand[0][0]             \n__________________________________________________________________________________________________\nblock_5_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_5_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_5_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_5_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_5_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_5_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_5_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_5_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_5_project (Conv2D)        (None, 28, 28, 32)   6144        block_5_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_5_project_BN (BatchNormal (None, 28, 28, 32)   128         block_5_project[0][0]            \n__________________________________________________________________________________________________\nblock_5_add (Add)               (None, 28, 28, 32)   0           block_4_add[0][0]                \n                                                                 block_5_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_6_expand (Conv2D)         (None, 28, 28, 192)  6144        block_5_add[0][0]                \n__________________________________________________________________________________________________\nblock_6_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_6_expand[0][0]             \n__________________________________________________________________________________________________\nblock_6_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_6_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_6_pad (ZeroPadding2D)     (None, 29, 29, 192)  0           block_6_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_6_depthwise (DepthwiseCon (None, 14, 14, 192)  1728        block_6_pad[0][0]                \n__________________________________________________________________________________________________\nblock_6_depthwise_BN (BatchNorm (None, 14, 14, 192)  768         block_6_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_6_depthwise_relu (ReLU)   (None, 14, 14, 192)  0           block_6_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_6_project (Conv2D)        (None, 14, 14, 64)   12288       block_6_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_6_project_BN (BatchNormal (None, 14, 14, 64)   256         block_6_project[0][0]            \n__________________________________________________________________________________________________\nblock_7_expand (Conv2D)         (None, 14, 14, 384)  24576       block_6_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_7_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_7_expand[0][0]             \n__________________________________________________________________________________________________\nblock_7_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_7_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_7_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_7_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_7_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_7_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_7_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_7_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_7_project (Conv2D)        (None, 14, 14, 64)   24576       block_7_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_7_project_BN (BatchNormal (None, 14, 14, 64)   256         block_7_project[0][0]            \n__________________________________________________________________________________________________\nblock_7_add (Add)               (None, 14, 14, 64)   0           block_6_project_BN[0][0]         \n                                                                 block_7_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_8_expand (Conv2D)         (None, 14, 14, 384)  24576       block_7_add[0][0]                \n__________________________________________________________________________________________________\nblock_8_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_8_expand[0][0]             \n__________________________________________________________________________________________________\nblock_8_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_8_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_8_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_8_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_8_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_8_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_8_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_8_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_8_project (Conv2D)        (None, 14, 14, 64)   24576       block_8_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_8_project_BN (BatchNormal (None, 14, 14, 64)   256         block_8_project[0][0]            \n__________________________________________________________________________________________________\nblock_8_add (Add)               (None, 14, 14, 64)   0           block_7_add[0][0]                \n                                                                 block_8_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_9_expand (Conv2D)         (None, 14, 14, 384)  24576       block_8_add[0][0]                \n__________________________________________________________________________________________________\nblock_9_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_9_expand[0][0]             \n__________________________________________________________________________________________________\nblock_9_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_9_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_9_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_9_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_9_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_9_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_9_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_9_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_9_project (Conv2D)        (None, 14, 14, 64)   24576       block_9_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_9_project_BN (BatchNormal (None, 14, 14, 64)   256         block_9_project[0][0]            \n__________________________________________________________________________________________________\nblock_9_add (Add)               (None, 14, 14, 64)   0           block_8_add[0][0]                \n                                                                 block_9_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_10_expand (Conv2D)        (None, 14, 14, 384)  24576       block_9_add[0][0]                \n__________________________________________________________________________________________________\nblock_10_expand_BN (BatchNormal (None, 14, 14, 384)  1536        block_10_expand[0][0]            \n__________________________________________________________________________________________________\nblock_10_expand_relu (ReLU)     (None, 14, 14, 384)  0           block_10_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_10_depthwise (DepthwiseCo (None, 14, 14, 384)  3456        block_10_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_10_depthwise_BN (BatchNor (None, 14, 14, 384)  1536        block_10_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           block_10_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_10_project (Conv2D)       (None, 14, 14, 96)   36864       block_10_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_10_project_BN (BatchNorma (None, 14, 14, 96)   384         block_10_project[0][0]           \n__________________________________________________________________________________________________\nblock_11_expand (Conv2D)        (None, 14, 14, 576)  55296       block_10_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_11_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_11_expand[0][0]            \n__________________________________________________________________________________________________\nblock_11_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_11_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_11_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_11_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_11_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_11_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_11_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_11_project (Conv2D)       (None, 14, 14, 96)   55296       block_11_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_11_project_BN (BatchNorma (None, 14, 14, 96)   384         block_11_project[0][0]           \n__________________________________________________________________________________________________\nblock_11_add (Add)              (None, 14, 14, 96)   0           block_10_project_BN[0][0]        \n                                                                 block_11_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_12_expand (Conv2D)        (None, 14, 14, 576)  55296       block_11_add[0][0]               \n__________________________________________________________________________________________________\nblock_12_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_12_expand[0][0]            \n__________________________________________________________________________________________________\nblock_12_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_12_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_12_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_12_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_12_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_12_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_12_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_12_project (Conv2D)       (None, 14, 14, 96)   55296       block_12_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_12_project_BN (BatchNorma (None, 14, 14, 96)   384         block_12_project[0][0]           \n__________________________________________________________________________________________________\nblock_12_add (Add)              (None, 14, 14, 96)   0           block_11_add[0][0]               \n                                                                 block_12_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_13_expand (Conv2D)        (None, 14, 14, 576)  55296       block_12_add[0][0]               \n__________________________________________________________________________________________________\nblock_13_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_13_expand[0][0]            \n__________________________________________________________________________________________________\nblock_13_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_13_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_13_pad (ZeroPadding2D)    (None, 15, 15, 576)  0           block_13_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_13_depthwise (DepthwiseCo (None, 7, 7, 576)    5184        block_13_pad[0][0]               \n__________________________________________________________________________________________________\nblock_13_depthwise_BN (BatchNor (None, 7, 7, 576)    2304        block_13_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_13_depthwise_relu (ReLU)  (None, 7, 7, 576)    0           block_13_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_13_project (Conv2D)       (None, 7, 7, 160)    92160       block_13_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_13_project_BN (BatchNorma (None, 7, 7, 160)    640         block_13_project[0][0]           \n__________________________________________________________________________________________________\nblock_14_expand (Conv2D)        (None, 7, 7, 960)    153600      block_13_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_14_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_14_expand[0][0]            \n__________________________________________________________________________________________________\nblock_14_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_14_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_14_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_14_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_14_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_14_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_14_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_14_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_14_project (Conv2D)       (None, 7, 7, 160)    153600      block_14_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_14_project_BN (BatchNorma (None, 7, 7, 160)    640         block_14_project[0][0]           \n__________________________________________________________________________________________________\nblock_14_add (Add)              (None, 7, 7, 160)    0           block_13_project_BN[0][0]        \n                                                                 block_14_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_15_expand (Conv2D)        (None, 7, 7, 960)    153600      block_14_add[0][0]               \n__________________________________________________________________________________________________\nblock_15_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_15_expand[0][0]            \n__________________________________________________________________________________________________\nblock_15_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_15_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_15_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_15_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_15_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_15_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_15_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_15_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_15_project (Conv2D)       (None, 7, 7, 160)    153600      block_15_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_15_project_BN (BatchNorma (None, 7, 7, 160)    640         block_15_project[0][0]           \n__________________________________________________________________________________________________\nblock_15_add (Add)              (None, 7, 7, 160)    0           block_14_add[0][0]               \n                                                                 block_15_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_16_expand (Conv2D)        (None, 7, 7, 960)    153600      block_15_add[0][0]               \n__________________________________________________________________________________________________\nblock_16_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_16_expand[0][0]            \n__________________________________________________________________________________________________\nblock_16_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_16_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_16_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_16_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_16_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_16_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_16_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_16_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_16_project (Conv2D)       (None, 7, 7, 320)    307200      block_16_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_16_project_BN (BatchNorma (None, 7, 7, 320)    1280        block_16_project[0][0]           \n__________________________________________________________________________________________________\nConv_1 (Conv2D)                 (None, 7, 7, 1280)   409600      block_16_project_BN[0][0]        \n__________________________________________________________________________________________________\nConv_1_bn (BatchNormalization)  (None, 7, 7, 1280)   5120        Conv_1[0][0]                     \n__________________________________________________________________________________________________\nout_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]                  \n__________________________________________________________________________________________________\nglobal_average_pooling2d (Globa (None, 1280)         0           out_relu[0][0]                   \n==================================================================================================\nTotal params: 2,257,984\nTrainable params: 2,223,872\nNon-trainable params: 34,112\n__________________________________________________________________________________________________", "name": "stdout"}]}, {"metadata": {"trusted": true}, "id": "3351b0ee", "cell_type": "code", "source": "def model_fn():\n    \"\"\"\n    Returns a MobileNetV2 model with top layer removed \n    and broadcasted pretrained weights.\n    \"\"\"\n    model = MobileNetV2(weights='imagenet',\n                        include_top=True,\n                        input_shape=(224, 224, 3))\n    for layer in model.layers:\n        layer.trainable = False\n    new_model = Model(inputs=model.input,\n                  outputs=model.layers[-2].output)\n    new_model.set_weights(brodcast_weights.value)\n    return new_model", "execution_count": 17, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {}, "id": "d44aabc0", "cell_type": "markdown", "source": "#### 4.10.5.3 D\u00e9finition du processus de chargement des images <br/> et application de leur featurisation \u00e0 travers l'utilisation de pandas UDF"}, {"metadata": {"scrolled": true, "trusted": true}, "id": "f5605ff0", "cell_type": "code", "source": "def preprocess(content):\n    \"\"\"\n    Preprocesses raw image bytes for prediction.\n    \"\"\"\n    img = Image.open(io.BytesIO(content)).resize([224, 224])\n    arr = img_to_array(img)\n    return preprocess_input(arr)\n\ndef featurize_series(model, content_series):\n    \"\"\"\n    Featurize a pd.Series of raw images using the input model.\n    :return: a pd.Series of image features\n    \"\"\"\n    input = np.stack(content_series.map(preprocess))\n    preds = model.predict(input)\n    # For some layers, output features will be multi-dimensional tensors.\n    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n    output = [p.flatten() for p in preds]\n    return pd.Series(output)\n\n@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\ndef featurize_udf(content_series_iter):\n    '''\n    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n\n    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n                              is a pandas Series of image data.\n    '''\n    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n    # for multiple data batches.  This amortizes the overhead of loading big models.\n    model = model_fn()\n    for content_series in content_series_iter:\n        yield featurize_series(model, content_series)", "execution_count": 18, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "/mnt/yarn/usercache/livy/appcache/application_1678634621736_0001/container_1678634621736_0001_01_000001/pyspark.zip/pyspark/sql/pandas/functions.py:392: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.", "name": "stdout"}]}, {"metadata": {}, "id": "0849a221", "cell_type": "markdown", "source": "#### 4.10.5.4 Ex\u00e9cutions des actions d'extractions de features"}, {"metadata": {"trusted": true}, "id": "e493cb49", "cell_type": "code", "source": "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")", "execution_count": 19, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "id": "8b0a0943", "cell_type": "code", "source": "features_df = images.repartition(24).select(col(\"path\"),\n                                            col(\"label\"),\n                                            featurize_udf(\"content\").alias(\"features\")\n                                           )", "execution_count": 20, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "id": "191f024f", "cell_type": "code", "source": "print(PATH_Result)", "execution_count": 21, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "s3://oc-p8-bucket/Results", "name": "stdout"}]}, {"metadata": {"trusted": true}, "id": "b771d8a9", "cell_type": "code", "source": "features_df.write.mode(\"overwrite\").parquet(PATH_Result)", "execution_count": 22, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "id": "fe11dd05", "cell_type": "code", "source": "features_df = pd.read_parquet(PATH_Result, engine='pyarrow')", "execution_count": 39, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "An error was encountered:\nmodule 'charset_normalizer' has no attribute 'md__mypyc'\nTraceback (most recent call last):\n  File \"/usr/local/lib64/python3.7/site-packages/pandas/io/parquet.py\", line 500, in read_parquet\n    **kwargs,\n  File \"/usr/local/lib64/python3.7/site-packages/pandas/io/parquet.py\", line 236, in read\n    mode=\"rb\",\n  File \"/usr/local/lib64/python3.7/site-packages/pandas/io/parquet.py\", line 84, in _get_path_or_handle\n    path_or_handle, **(storage_options or {})\n  File \"/usr/local/lib/python3.7/site-packages/fsspec/core.py\", line 353, in url_to_fs\n    chain = _un_chain(url, kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/fsspec/core.py\", line 315, in _un_chain\n    cls = get_filesystem_class(protocol)\n  File \"/usr/local/lib/python3.7/site-packages/fsspec/registry.py\", line 211, in get_filesystem_class\n    register_implementation(protocol, _import_class(bit[\"class\"]))\n  File \"/usr/local/lib/python3.7/site-packages/fsspec/registry.py\", line 234, in _import_class\n    mod = importlib.import_module(mod)\n  File \"/usr/lib64/python3.7/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"/usr/local/lib/python3.7/site-packages/s3fs/__init__.py\", line 1, in <module>\n    from .core import S3FileSystem, S3File\n  File \"/usr/local/lib/python3.7/site-packages/s3fs/core.py\", line 28, in <module>\n    import aiobotocore.session\n  File \"/usr/local/lib/python3.7/site-packages/aiobotocore/session.py\", line 12, in <module>\n    from .client import AioBaseClient, AioClientCreator\n  File \"/usr/local/lib/python3.7/site-packages/aiobotocore/client.py\", line 20, in <module>\n    from .args import AioClientArgsCreator\n  File \"/usr/local/lib/python3.7/site-packages/aiobotocore/args.py\", line 8, in <module>\n    from .endpoint import AioEndpointCreator\n  File \"/usr/local/lib/python3.7/site-packages/aiobotocore/endpoint.py\", line 19, in <module>\n    from aiobotocore.httpsession import AIOHTTPSession\n  File \"/usr/local/lib/python3.7/site-packages/aiobotocore/httpsession.py\", line 7, in <module>\n    import aiohttp  # lgtm [py/import-and-import-from]\n  File \"/usr/local/lib64/python3.7/site-packages/aiohttp/__init__.py\", line 6, in <module>\n    from .client import (\n  File \"/usr/local/lib64/python3.7/site-packages/aiohttp/client.py\", line 59, in <module>\n    from .client_reqrep import (\n  File \"/usr/local/lib64/python3.7/site-packages/aiohttp/client_reqrep.py\", line 72, in <module>\n    import charset_normalizer as chardet  # type: ignore[no-redef]\n  File \"/usr/local/lib64/python3.7/site-packages/charset_normalizer/__init__.py\", line 24, in <module>\n    from .api import from_bytes, from_fp, from_path, normalize\n  File \"/usr/local/lib64/python3.7/site-packages/charset_normalizer/api.py\", line 10, in <module>\n    from .cd import (\n  File \"/usr/local/lib64/python3.7/site-packages/charset_normalizer/cd.py\", line 9, in <module>\n    from .md import is_suspiciously_successive_range\nAttributeError: module 'charset_normalizer' has no attribute 'md__mypyc'\n\n", "name": "stderr"}]}, {"metadata": {}, "id": "db3c9754", "cell_type": "markdown", "source": "### 4.10.6 R\u00e9duction de dimension (PCA)"}, {"metadata": {"trusted": true}, "id": "a174e6ad", "cell_type": "code", "source": "from pyspark.ml.feature import PCA\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\nfrom charset_normalizer import md__mypyc", "execution_count": 40, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "An error was encountered:\nmodule 'charset_normalizer' has no attribute 'md__mypyc'\nTraceback (most recent call last):\n  File \"/usr/local/lib64/python3.7/site-packages/charset_normalizer/__init__.py\", line 24, in <module>\n    from .api import from_bytes, from_fp, from_path, normalize\n  File \"/usr/local/lib64/python3.7/site-packages/charset_normalizer/api.py\", line 10, in <module>\n    from .cd import (\n  File \"/usr/local/lib64/python3.7/site-packages/charset_normalizer/cd.py\", line 9, in <module>\n    from .md import is_suspiciously_successive_range\nAttributeError: module 'charset_normalizer' has no attribute 'md__mypyc'\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "id": "37563e64", "cell_type": "code", "source": "%%configure -f", "execution_count": 43, "outputs": [{"output_type": "stream", "text": "UsageError: %%configure is a cell magic, but the cell body is empty.\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "id": "1709d7b9", "cell_type": "code", "source": "sc.list_packages()", "execution_count": 42, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "An error was encountered:\nlist_packages can only use called when spark.pyspark.virtualenv.enabled is set to true\nTraceback (most recent call last):\n  File \"/mnt/yarn/usercache/livy/appcache/application_1678634621736_0001/container_1678634621736_0001_01_000001/pyspark.zip/pyspark/context.py\", line 1344, in list_packages\n    raise RuntimeError(\"list_packages can only use called when \"\nRuntimeError: list_packages can only use called when spark.pyspark.virtualenv.enabled is set to true\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "id": "798e769b", "cell_type": "code", "source": "SparkContext.addPyFile(\"module.py\") pip install charset-normalizer", "execution_count": 41, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "An error was encountered:\ninvalid syntax (<stdin>, line 1)\n  File \"<stdin>\", line 1\n    pip install charset-normalizer\n              ^\nSyntaxError: invalid syntax\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "id": "93b9e488", "cell_type": "code", "source": "def update_schema(df):\n    \n    # Define a UDF to convert the features column to a list\n    to_list_udf = udf(lambda row: [float(i) for i in row], ArrayType(FloatType()))\n\n    # Convert the features column to a list and create a new column \"features_list\"\n    features_df = features_df.withColumn(\"features_list\", to_list_udf(features_df[\"features\"]))\n\n    # Define UDF to convert list to DenseVector\n    to_dense_vector_udf = udf(lambda x: Vectors.dense(x), VectorUDT())\n\n    # Convert the \"features_list\" column to DenseVector and create a new column \"features_vec\"\n    features_df = features_df.withColumn(\"features_vec\", to_dense_vector_udf(features_df[\"features_list\"]))\n\n    # Drop the \"features_list\" column\n    features_df = features_df.drop(\"features_list\")\n    \n    return features_df\n\ndef scaler(inputCol, outputCol):\n    \n    scaler = StandardScaler(\n        inputCol = 'features_vec', \n        outputCol = 'scaledFeatures',\n        withMean = True,\n        withStd = True\n    ).fit(features_df)\n\n    # when we transform the dataframe, the old\n    # feature will still remain in it\n    features_df = scaler.transform(features_df)\n\n    return features_df\n\ndef PCA(inputCol, outputCol):\n    \n    n_components = 2\n    pca = PCA(\n        k = n_components, \n        inputCol = 'scaledFeatures', \n        outputCol = 'pcaFeatures'\n    ).fit(df_scaled)\n    \n    df_pca = pca.transform(df_scaled).select('path', 'label', 'pcaFeatures')\n    \n    return df_pca\n\ndef export_Parquet(df, export_path):\n    \n    df_pca.write.mode(\"overwrite\").parquet(PATH_Result)\n    df_pca = pd.read_parquet(PATH_Result, engine='pyarrow')\n    \n    return df_pca", "execution_count": 28, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "id": "85f3fa9c", "cell_type": "code", "source": "df = update_schema(features_df)\n", "execution_count": 29, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "An error was encountered:\nlocal variable 'features_df' referenced before assignment\nTraceback (most recent call last):\n  File \"<stdin>\", line 7, in update_schema\nUnboundLocalError: local variable 'features_df' referenced before assignment\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "id": "e5647e99", "cell_type": "code", "source": "standard_scaler = scaler(inputCol='features_vec', \n                        outputCol='scaledFeatures')", "execution_count": 30, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "An error was encountered:\nname 'StandardScaler' is not defined\nTraceback (most recent call last):\n  File \"<stdin>\", line 22, in scaler\nNameError: name 'StandardScaler' is not defined\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "id": "8eea7620", "cell_type": "code", "source": "\n\n\npca_extractor = PCA(inputCol='scaledFeatures', \n                    outputCol='pcaFeatures')\n\npipeline = Pipeline(stages=[standard_scaler, pca_extractor])\n\nPCA = pipeline.fit(df)\n\ndf_pca = export_Parquet(df=PCA,\n                       export_path=PATH_Result)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "2acab85d", "cell_type": "markdown", "source": "### 4.10.7 Chargement des donn\u00e9es enregistr\u00e9es et validation du r\u00e9sultat"}, {"metadata": {"trusted": true}, "id": "d546218f", "cell_type": "code", "source": "df = pd.read_parquet(PATH_Result, engine='pyarrow')", "execution_count": 18, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "", "version_major": 2, "version_minor": 0}, "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"}, "metadata": {}, "output_type": "display_data"}]}, {"metadata": {"trusted": true}, "id": "cebb37ee", "cell_type": "code", "source": "df.head()", "execution_count": 19, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "", "version_major": 2, "version_minor": 0}, "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "                                           path  ...                                           features\n0    s3://p8-data/Test/Watermelon/r_174_100.jpg  ...  [0.0059991637, 0.44703647, 0.0, 0.0, 3.3713572...\n1  s3://p8-data/Test/Pineapple Mini/128_100.jpg  ...  [0.0146466885, 4.080593, 0.055877004, 0.0, 0.0...\n2  s3://p8-data/Test/Pineapple Mini/137_100.jpg  ...  [0.0, 4.9659867, 0.0, 0.0, 0.0, 0.0, 0.5144821...\n3      s3://p8-data/Test/Watermelon/275_100.jpg  ...  [0.22511952, 0.07235509, 0.0, 0.0, 1.690149, 0...\n4      s3://p8-data/Test/Watermelon/271_100.jpg  ...  [0.3286234, 0.18830013, 0.0, 0.0, 1.9123534, 0...\n\n[5 rows x 3 columns]"}]}, {"metadata": {"scrolled": true, "trusted": true}, "id": "fa366e1a", "cell_type": "code", "source": "df.loc[0,'features'].shape", "execution_count": 20, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "", "version_major": 2, "version_minor": 0}, "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "(1280,)"}]}, {"metadata": {"trusted": true}, "id": "bdbbea62", "cell_type": "code", "source": "df.shape", "execution_count": 21, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "", "version_major": 2, "version_minor": 0}, "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "(22688, 3)"}]}, {"metadata": {}, "id": "570281cb", "cell_type": "markdown", "source": "<u>On peut \u00e9galement constater la pr\u00e9sence des fichiers <br />\n    au format \"**parquet**\" sur le **serveur S3**</u> :\n\n![Affichage des r\u00e9sultats sur S3](img/S3_Results.png)\n\n## 4.11 Suivi de l'avancement des t\u00e2ches avec le Serveur d'Historique Spark\n\nIl est possible de voir l'avancement des t\u00e2ches en cours <br />\navec le **serveur d'historique Spark**.\n\n![Acc\u00e8s au serveur d'historique spark](img/EMR_serveur_historique_spark_acces.png)\n\n**Il est \u00e9galement possible de revenir et d'\u00e9tudier les t\u00e2ches <br />\nqui ont \u00e9t\u00e9 r\u00e9alis\u00e9, afin de debugger, optimiser les futurs <br />\nt\u00e2ches \u00e0 r\u00e9aliser.**\n\n<u>Lorsque la commande \"**features_df.write.mode(\"overwrite\").parquet(PATH_Result)**\" <br />\n\u00e9tait en cours, nous pouvions observer son \u00e9tat d'avancement</u> :\n\n![Progression execution script](img/EMR_jupyterhub_avancement.png)\n\n<u>Le **serveur d'historique Spark** nous permet une vision beaucoup plus pr\u00e9cise <br />\nde l'ex\u00e9cution des diff\u00e9rentes t\u00e2che sur les diff\u00e9rentes machines du cluster</u> :\n\n![Suivi des t\u00e2ches spark](img/EMR_SHSpark_01.png)\n\nOn peut \u00e9galement constater que notre cluster de calcul a mis <br />\nun tout petit peu **moins de 8 minutes** pour traiter les **22 688 images**.\n\n![Temps de traitement](img/EMR_SHSpark_02.png)\n"}, {"metadata": {}, "id": "c4030775", "cell_type": "markdown", "source": "## 4.12 R\u00e9siliation de l'instance EMR\n\nNotre travail est maintenant termin\u00e9. <br />\nLe cluster de machines EMR est **factur\u00e9 \u00e0 la demande**, <br />\net nous continuons d'\u00eatre factur\u00e9 m\u00eame lorsque <br />\nles machines sont au repos.<br />\nPour **optimiser la facturation**, il nous faut <br />\nmaintenant **r\u00e9silier le cluster**.\n\n<u>Je r\u00e9alise cette commande depuis l'interface AWS</u> :\n\n1. Commencez par **d\u00e9sactiver le tunnel ssh dans FoxyProxy** pour \u00e9viter des probl\u00e8mes de **timeout**.\n![D\u00e9sactivation de FoxyProxy](img/EMR_foxyproxy_desactivation.png)\n2. Cliquez sur \"**R\u00e9silier**\"\n![Cliquez sur R\u00e9silier](img/EMR_resiliation_01.png)\n3. Confirmez la r\u00e9siliation\n![Confirmez la r\u00e9siliation](img/EMR_resiliation_02.png)\n4. La r\u00e9siliation prend environ **1 minute**\n![R\u00e9siliation en cours](img/EMR_resiliation_03.png)\n5. La r\u00e9siliation est effectu\u00e9e\n![R\u00e9siliation termin\u00e9e](img/EMR_resiliation_04.png)\n\n## 4.13 Cloner le serveur EMR (si besoin)\n\nSi nous devons de nouveau ex\u00e9cuter notre notebook dans les m\u00eames conditions, <br />\nil nous suffit de **cloner notre cluster** et ainsi en obtenir une copie fonctionnelle <br />\nsous 15/20 minutes, le temps de son instanciation.\n\n<u>Pour cela deux solutions</u> :\n1. <u>Depuis l'interface AWS</u> :\n 1. Cliquez sur \"**Cloner**\"\n   ![Cloner un cluster](img/EMR_cloner_01.png)\n 2. Dans notre cas nous ne souhaitons pas inclure d'\u00e9tapes\n   ![Ne pas inclure d'\u00e9tapes](img/EMR_cloner_02.png)\n 3. La configuration du cluster est recr\u00e9\u00e9e \u00e0 l\u2019identique. <br />\n    On peut revenir sur les diff\u00e9rentes \u00e9tapes si on souhaite apporter des modifications<br />\n    Quand tout est pr\u00eat, cliquez sur \"**Cr\u00e9er un cluster**\"\n  ![V\u00e9rification/Modification/Cr\u00e9er un cluster](img/EMR_cloner_03.png)\n2. <u>En ligne de commande</u> (avec AWS CLI d'install\u00e9 et de configur\u00e9 et en s'assurant <br />\n   de s'attribuer les droits n\u00e9cessaires sur le compte AMI utilis\u00e9)\n 1. Cliquez sur \"**Exporter AWS CLI**\"\n ![Exporter AWS CLI](img/EMR_cloner_cli_01.png)\n 2. Copier/Coller la commande **depuis un terminal**\n ![Copier Coller Commande](img/EMR_cloner_cli_02.png)\n\n## 4.14 Arborescence du serveur S3 \u00e0 la fin du projet\n\n<u>Pour information, voici **l'arborescence compl\u00e8te de mon bucket S3 p8-data** \u00e0 la fin du projet</u> : <br />\n*Par soucis de lisibilit\u00e9, je ne liste pas les 131 sous dossiers du r\u00e9pertoire \"Test\"*\n\n1. Results/_SUCCESS\n1. Results/part-00000-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00001-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00002-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00003-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00004-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00005-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00006-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00007-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00008-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00009-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00010-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00011-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00012-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00013-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00014-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00015-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00016-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00017-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00018-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00019-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00020-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00021-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00022-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Results/part-00023-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n1. Test/\n1. bootstrap-emr.sh\n1. jupyter-s3-conf.json\n1. jupyter/jovyan/.s3keep\n1. jupyter/jovyan/P8_01_Notebook.ipynb\n1. jupyter/jovyan/_metadata\n1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/\n1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/file-perm.sqlite\n1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/\n1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/\n1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/html/\n1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/latex/\n1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbsignatures.db\n1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/notebook_secret\n1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/\n1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/Untitled-checkpoint.ipynb\n1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/Untitled1-checkpoint.ipynb\n1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/test3-checkpoint.ipynb\n1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/Untitled.ipynb\n1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/Untitled1.ipynb\n1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/test3.ipynb"}, {"metadata": {}, "id": "594f343e", "cell_type": "markdown", "source": "# 5. Conclusion\n\nNous avons r\u00e9alis\u00e9 ce projet **en deux temps** en tenant <br />\ncompte des contraintes qui nous ont \u00e9t\u00e9 impos\u00e9es.\n\nNous avons **dans un premier temps d\u00e9velopp\u00e9 notre solution en local** <br />\nsur une machine virtuelle dans un environnement Linux Ubuntu.\n\nLa <u>premi\u00e8re phase</u> a consist\u00e9 \u00e0 **installer l'environnement de travail Spark**. <br />\n**Spark** a un param\u00e8tre qui nous permet de travaill\u00e9 en local et nous permet <br />\nainsi de **simuler du calcul partag\u00e9** en consid\u00e9rant <br />\n**chaque c\u0153ur d'un processeur comme un worker ind\u00e9pendant**.<br />\nNous avons travaill\u00e9 sur un plus **petit jeu de donn\u00e9e**, l'id\u00e9e \u00e9tait <br />\nsimplement de **valider le bon fonctionnement de la solution**.\n\nNous avons fait le choix de r\u00e9aliser du **transfert learning** <br />\n\u00e0 partir du model **MobileNetV2**.<br />\nCe mod\u00e8le a \u00e9t\u00e9 retenu pour sa **l\u00e9g\u00e8ret\u00e9** et sa **rapidit\u00e9 d'ex\u00e9cution** <br />\nainsi que pour la **faible dimension de son vecteur en sortie**.\n\nLes r\u00e9sultats ont \u00e9t\u00e9 enregistr\u00e9s sur disque en plusieurs <br />\npartitions au format \"**parquet**\".\n\n<u>**La solution a parfaitement fonctionn\u00e9 en mode local**</u>.\n\nLa <u>deuxi\u00e8me phase</u> a consist\u00e9 \u00e0 cr\u00e9er un **r\u00e9el cluster de calculs**. <br />\nL'objectif \u00e9tait de pouvoir **anticiper une future augmentation de la charge de travail**.\n\nLe meilleur choix retenu a \u00e9t\u00e9 l'utilisation du prestataire de services **Amazon Web Services** <br />\nqui nous permet de **louer \u00e0 la demande de la puissance de calculs**, <br />\npour un **co\u00fbt tout \u00e0 fait acceptable**.<br />\nCe service se nomme **EC2** et se classe parmi les offres **Infrastructure As A Service** (IAAS).\n\nNous sommes allez plus loin en utilisant un service de plus <br />\nhaut niveau (**Plateforme As A Service** PAAS)<br />\nen utilisant le service **EMR** qui nous permet d'un seul coup <br />\nd'**instancier plusieurs serveur (un cluster)** sur lesquels <br />\nnous avons pu demander l'installation et la configuration de plusieurs<br />\nprogrammes et librairies n\u00e9cessaires \u00e0 notre projet comme **Spark**, <br />\n**Hadoop**, **JupyterHub** ainsi que la librairie **TensorFlow**.\n\nEn plus d'\u00eatre plus **rapide et efficace \u00e0 mettre en place**, nous avons <br />\nla **certitude du bon fonctionnement de la solution**, celle-ci ayant \u00e9t\u00e9 <br />\npr\u00e9alablement valid\u00e9 par les ing\u00e9nieurs d'Amazon.\n\nNous avons \u00e9galement pu installer, sans difficult\u00e9, **les packages <br />\nn\u00e9cessaires sur l'ensembles des machines du cluster**.\n\nEnfin, avec tr\u00e8s peu de modification, et plus simplement encore, <br />\nnous avons pu **ex\u00e9cuter notre notebook comme nous l'avions fait localement**.<br />\nNous avons cette fois-ci ex\u00e9cut\u00e9 le traitement sur **l'ensemble des images de notre dossier \"Test\"**.\n\nNous avons opt\u00e9 pour le service **Amazon S3** pour **stocker les donn\u00e9es de notre projet**. <br />\nS3 offre, pour un faible co\u00fbt, toutes les conditions dont nous avons besoin pour stocker <br />\net exploiter de mani\u00e8re efficace nos donn\u00e9es.<br />\nL'espace allou\u00e9 est potentiellement **illimit\u00e9**, mais les co\u00fbts seront fonction de l'espace utilis\u00e9.\n\nIl nous sera **facile de faire face \u00e0 une mont\u00e9 de la charge de travail** en **redimensionnant** <br />\nsimplement notre cluster de machines (horizontalement et/ou verticalement au besoin), <br />\nles co\u00fbts augmenteront en cons\u00e9quence mais resteront nettement inf\u00e9rieurs aux co\u00fbts engendr\u00e9s <br />\npar l'achat de mat\u00e9riels ou par la location de serveurs d\u00e9di\u00e9s."}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": "python"}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 3}, "file_extension": ".py", "pygments_lexer": "python3"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {"height": "calc(100% - 180px)", "left": "10px", "top": "150px", "width": "256px"}, "toc_section_display": true, "toc_window_display": true}}, "nbformat": 4, "nbformat_minor": 5}